\documentclass[12pt, letterpaper]{article}
%\usepackage[top=2cm,bottom=4cm,left=1.5cm,right=3cm,asymmetric]{geometry} aggiungere ^twoside^
\usepackage{fancyhdr}
\pagestyle{fancy}

 \fancyfoot[C]{}    
 \fancyfoot[LE,RO]{\thepage}        
 \fancyhead[RO]{\slshape \rightmark}        
 \fancyhead[LE]{\slshape\leftmark}      
  \fancyhead[RE,LO]{}  

%%%%%
\usepackage{xcolor}
\usepackage{listings}

\definecolor{mGreen}{rgb}{0,0.6,0}
\definecolor{mGray}{rgb}{0.5,0.5,0.5}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\definecolor{backgroundColour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{CStyle}{
    backgroundcolor=\color{backgroundColour},   
    commentstyle=\color{mGreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{mGray},
    stringstyle=\color{mPurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=C
}
%%%%%
\usepackage{color}   %May be necessary if you want to color links
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=black,  %choose some color if you want links to stand out
}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\newcommand{\img}[3] {
	\begin{figure}[h]
		\caption{#1}
		\centering
		\includegraphics[scale=#2]{#3}\\
	\end{figure}
}

\title{La Bibbia di Sistemi operativi}
\author{Mario Petruccelli \cr Università degli studi di Milano}
\date{A.A. 2018/2019}

\addto\captionsenglish{% Replace "english" with the language you use
  \renewcommand{\contentsname}
    {Sommario}
}

\begin{document}

	\begin{titlepage} \maketitle \newpage \tableofcontents \end{titlepage}
	
	\section{Virtualization}
		
		\subsection{Introduzione}
		
			\paragraph{Processi} Un processo, informalmente, è un programma in esecuzione. Un programma a sua volta, è una sequenza finita di istruzioni scritte in un linguaggio comprensibile all'esecutore (CPU).
				L'esecuzione di un programma da parte del processore è:
				\begin{itemize}
					\item \textbf{Fetch} Prelievo istruzione dalla memoria.
					\item \textbf{Decode} Decodifica dell'istruzione.
					\item \textbf{Execute} Esecuzione dell'istruzione.
				\end{itemize}
			
			\subsubsection{Virtualizzazione} La virtualizzazione consiste nel prendere una risorsa fisica e trasformarla in una più generale, potente e facile da adoperare forma virtuale di se stessa. 
			
				\paragraph{Virtualizzazione della CPU} L'illusione consiste nel far credere che il sistema abbia un elevato numero di cpu virtuali. Avere più CPU permeterrebbe a più programmi di essere eseguiti in \textbf{parallelo} nonostante il processore fisico effettivo sia uno solo. Se due processi vogliono essere eseguiti entrambi ad un certo tempo, oppure vogliono accedere alla stessa periferica, quale dei due ha la priorità? La risposta viene data con l'introduzione delle politiche di priorità (\textbf{politiche di scheduling}).
			
				\paragraph{Virtualizzazione della memoria} Consiste nel fabbricare l'illusione che ogni processo abbia il proprio spazio di indirizzi virtuali privato (\textbf{address space}) al quale accede e sarà il sistema operativo ad occuparsi di mappare nella memoria fisica.\\
						
				Con la virtualizzazione è fondamentale riuscire a distinguere i processi in esecuzione. Per fare ciò viene associato un \textbf{PID} (process id) ad ogni job. il PID è un numero univoco.
		
			\subsubsection{Concorrenza} Si riferisce a tutta quelle serie di problemi che sorgono, e che vanno risolti, quando all'interno dello stesso programma più entità lavorano in parallelo. Le entità in questione si chiamano \textbf{threads}.
			
			\subsubsection{Persistenza} La persistenza è legata alla memorizzazione dei dati all'interno della memoria. La non volatilità delle memorie ha introdotto la possibilità di memorizzare dati in modo persistente. Il software nel sistema operativo che generalmente gestisce i dischi è chiamato \textbf{file system}.
			
			\subsubsection{Protezione ad anelli}
				Un modello di potezione implementato dal sistema operativo è quello ad anelli. Ci sono 5 livelli e 3 anelli differenti. A ciascun anello corrisponde un relativo livello di sicurezza. 
				\begin{itemize}
					\item \textbf{Level 1}\textit{ Hardware level} qui vengono eseguiti, ad esempio, i device drivers visto che essi richiedono accesso diretto all'hardware dei dispositivi (microcontroller).
					\item \textbf{Level 2}\textit{ Firmware level} Il  firmware sta in cima al livello elettronico. Contiene in software necessario dal dispositivo hardware e dal microcontroller. 
					\item \textbf{Level 3: ring 0}\textit{ Kernel level} Questo è il livello dove opera il kernel, dopo la fase di bootload siamo qui.
					\item \textbf{Level 4: ring 1 e 2}\textit{ Device drivers} I device drivers passano atraverso il kernel per accedere all'hardware.
					\item \textbf{Level 5: ring 3}\textit{ Application level} Qui è dove viene eseguito normalemente il codice utente.
				\end{itemize}
						
		\subsection{Processi}
			
			\paragraph{Sistema multiprogrammato} Sistema nel quale è possibile eseguire più programmi contemporaneamente, idea alla base della virtualizzazione.
			
			\subsubsection{Multiprogrammazione}
				
				\paragraph{Time sharing} prevede che il tempo di CPU sia equamente diviso fra i programmi in memoria. 
				\paragraph{Real time sharing} La politica di scheduling è differente. Alcuni processi vanno serviti prima di altri.
				
			\subsubsection{Virtualizzazione della CPU}
				L'illusione consiste nel rendere indipendenti il numero di processi dal numero di processori. Si vuole disaccoppiare le entità logiche (\textit{processi}), dalle entità fisiche (\textit{processori}), in modo tale che ad ogni processo venga assegnato un processore logico mappato su processore fisico.
				
				I concetti fondamentali alla base della virtualizzazione sono: 
				\begin{itemize}
					\item \textbf{Time sharing} Meccanismo mediante il quale il tempo di CPU viene diviso equamente fra i processi.
					\item \textbf{Context switch} Meccanismo che consente di interrompere l'esecuzione di un processo in corso sulla CPU fisica e assegnare quest'ultima ad un nuovo processo.
				\end{itemize}				 
				
			\subsubsection{Processi} Un processo è un programma in esecuzione, il sistema operativo deve fornire alcune interfacce (\textbf{APIs}) per la gestione dei processi che permettano di fare:
			
				\begin{itemize}
					\item \textbf{Create} Creazione di un nuovo processo.
					\item \textbf{Destroy} Eliminazione forzata di un processo. Molti processi termineranno per conto loro, ma l'utente potrebbe voler eliminare processi non ancora terminati.
					\item \textbf{Wait} Mette in attesa un processo. 
					\item \textbf{Miscellaneous control} Sospensione di un processo per farlo ripartire dopo un certo tempo.
					\item \textbf{Status} Interfacce che restituiscono lo stato e altre informazioni di un processo.
				\end{itemize}
				
				\paragraph{Creazione di un processo} La prima cosa che deve fare il sistema operativo per eseguire un programma è caricare il suo codice ed eventuali dati statici da disco a memoria, nell'address space del processo.
					\begin{itemize}
						\item \textbf{Allocazione dello stack} Un po' di memoria deve essere creata per lo stack del programma 	(\textit{variabili locali, parametri delle funzioni e indirizzi di ritorno}).	
						\item \textbf{Allocazione dello heap}  Un po' di memoria deve essere creata per lo heap del programma (\textit{dati allocati dinamicamente}).
						\item \textbf{Inizializzazione I/O} Standard input, output ed error.
						\item \textbf{Salto ed esecuzione} Salto all'entry point ed esecuzione. (\textit{main}) 
					\end{itemize}
					
				\paragraph{Stato di un processo}		
					\begin{itemize}
						\item \textbf{Running} È in esecuzione sul processore.
						\item \textbf{Ready} In attesa di essere eseguito dal processore.
						\item \textbf{Blocked} In stato di block, il processo sta esegundo qualche operazione (\textit{es: I/O}).
					\end{itemize}
					
				\paragraph{Strutture dati} Il sistema operativo deve tenere traccia delle informazioni fondamentali di un processo per poter ripristinare l'esecuzione di un processo interrotto. Esse sono: 
					\begin{itemize}
						\item Porzioni di memoria coinvolte.
						\item Valori dei registri di CPU usati dal processo.
						\item Stato dei dispositivi di I/O usati dal processo.
					\end{itemize}
					Questi dati sono organizzati in strutture chiamate \textbf{Process Control Block (PCB)}, salvate in un per-process \textbf{kernel stack}, il quale risiede nel kernel space.
					
			\subsubsection{Process API}
				
				La creazione di un processo avviene tramite la \texttt{fork()}, la quale genera un processo identico a quello in esecuzione. Tale processo prende il nome di padre, quello generato viene chiamato figlio. L'esecuzione del processo figlio parte dall'istruzione successiva alla \texttt{fork()}. La \texttt{fork()} ritorna al figlio 0, al padre il \textbf{PID} del figlio e \textbf{-1} in caso di errore. Il processo figlio avrà il \textbf{proprio} address space, registri, PC, ecc\dots   \\
				
				La \texttt{wait()} è una funzione che forza il padre ad aspettare che il processo figlio termini la propria esecuzione. Senza, l'output potrebbe essere \textbf{non-deterministico} e potrebbero crearsi processi orfani o zombie. Esiste anche la \texttt{waitpid} che viene usata se si ha a che fare più di un figlio.\\
				
				Per eliminare un processo esiste la funzione \texttt{kill()}. Solo il padre può distruggere il figlio. Ciò può portare alla creazione di processi \textbf{zombie} (processi terminati la cui \textbf{PCB} è ancora in memoria). \\
				
				La \texttt{exec()} serve per generare un processo che fa qualcosa di diverso da quello padre. \texttt{exec(nome\_programma, arg)} prende il nome di un eseguibile e alcuni argomenti, carica il codice e i dati statici di quell'eseguibile, sovrascrivendo il code segment corrente all'interno del PCB del figlio. Heap, stack e altre parti di memoria vengono re-inizializzate. Rimane la relazione padre-figlio.
				
		\subsection{Context Switch}
		
			\subsubsection{Shell}
				
				Come mai \texttt{fork()} ed \texttt{exec()} sono due system call separate? Per rispondere introduciamo la \textbf{shell}.
				
				La shell è un programma del sistema operativo \texttt{Unix }il cui compito è riconoscere ed eseguire altri programmi; si può dire che essa sia il genitore di tutti i processi che vengono mandati in esecuzione. Nello specifico, essa esegue una \texttt{fork()}, cambia il file descriptor se richiesto, ed infine invoca la \texttt{exec()}. Poi si mette in attesa che il programma abbia terminato prima di tornare in attesa di istruzioni. Esistono due tipi di shell, grafica (\textit{terminale}) e interattiva (\textit{aprire programmi col mouse}). 
				
				La separazione di \texttt{fork()} ed \texttt{exec()} è dovuta alla presenza della shell, con la quale possiamo andare ad effettuare alcune modifiche dopo la \texttt{fork()} e prima dell'\texttt{exec()}, come ad esempio la sostituzione del file descriptor. 
				
				$$\texttt{\$> wc file.c > n.txt}$$
				
				La shell esegue la \texttt{fork()} per poter mandare in esecuzione il programma \texttt{wc}. Prima di sostituire il codice del padre all'interno del PCB del figlio, sostituisce il file descriptor relativo allo standard output con n.txt. Successivamente esegue l'\texttt{exec()} producendo l'output desiderato all'interno di n.txt. Queste manipolazioni non sarebbero possibili se \texttt{fork()} ed \texttt{exec()} fossero un'unica system call perchè non si avrebbe accesso al PCB del figlio prima dell'exec().
				
			\subsubsection{Direct execution}
			
				Il concetto di direct execution è semplice: il programma viene eseguito direttamente sulla CPU fisica.
				
				Quando il sistema operativo desidera iniziare l'esecuzione di un programma, viene fatto quanto segue: 
				
				\begin{itemize}
					\item Crea una entry nella lista dei processi.
					\item Alloca la memoria per il programma.
					\item Carica il programma in memoria.
					\item Imposta lo stack con \texttt{argc/argv}.
					\item Pulisce i registri.
					\item Esegue la chiamata a \texttt{main()}.\\
					Si ha un salto dalla zona kernel al \texttt{main}. Il processo a questo punto deve:
					\item Eseguire il codice del \texttt{main()}.
					\item Ritornare dal main a fine esecuzione.\\
					Dal processo si torna alla zona kernel. Il sistema operativo infine:
					\item Rimuove la entry dalla lista dei processi.
				\end{itemize}
				
				Tuttavia la direct execution solleva alcune problematiche:
				
				\begin{itemize}
					\item Il sistema operativo non può assicurarsi che un programma in esecuzione non faccia qualcosa che non dovrebbe fare.
					\item Il sistema operativo non può fermare un processo in esecuzione.
				\end{itemize}
				
				Il primo problema si risolve con l'introduzione dello \textbf{user mode}. Il codice che viene eseguito in questa modalità di elaborazione è limitato in termini di istruzioni eseguibili. Nasce quindi anche la \textbf{kernel mode}, modalità in cui opera il sistema operativo e che consente di eseguire tutte le istruzioni privilegiate.
				
				Per permettere ad un processo di eseguire istruzioni privilegiate vengono introdotte delle \textbf{system call}. Per eseguirle, un programa deve eseguire un'istruzione \textbf{trap} (\textit{interrupt via software}).	Questa istruzione salta nel kernel, aumenta i privilegi a kernel mode, esegue le operazioni privilegiate e ritorna al processo scalando i privilegi tramite un'istruzione \textbf{return-from-trap}. Durante questo procedimento bisogna assicurarsi di salvare i registri del chiamante. Per sapere dove la trap deve saltare, il kernel imposta una \textbf{trap table} al boot time. Non è il processo utente a specificare l'indirizzo dei \textbf{trap handlers} perchè potrebbe saltare ovunque nel sistema.\\
				Per specificare la system call, generalmente viene assegnato un \textbf{system-call-number} che solitamente viene inserito in un registro appropriato.
				
			\subsubsection{Switch tra processi}
				
				\paragraph{Cooperative approach} Soluzione via software che consiste nel programmare il processo in modo che, dopo un certo numero di secondi di utilizzo della CPU, il comando torni al sistema operativo. Il problema è che se vengono creati loop infiniti nel programma, la CPU non verrebbe mai condivisa.
				
				\paragraph{Time interrupt} Soluzione via hardware che consiste nel creare una nuova componente che genera un segnale elettrico (\textbf{time interrupt}) dopo un certo lasso di tempo. Ci sarà quindi un orologio interno che invierà un segnale al piedino del microprocessore. L'hardware deve inoltre fermare l'esecuzione del processo corrente, salvarne lo stato per dare il controllo allo \textbf{scheduler}, che nel caso decidesse di cambiare processo, farà eseguire al sistema operativo codice a basso livello che prende il nome di \textbf{context switch}. 
				
				\paragraph{Context switch} Ciò che deve fare il sistema operativo è salvare alcuni valori dei registri per il processo in corso di esecuzione (\textit{nel kernel stack}) e ripristinarne altri per il processo scelto. Viene eseguita una return-from-trap per mandare in esecuzione il processo scelto.\\
				Interrupt, system call ed eccezioni sono eventi che inducono il mode switch. 

		\subsection{Scheduling policy}
			Dati $n$ processi, a quale assegno il processore?
			La scelta è fatta dallo \textbf{scheduler}, un modulo del sistema operativo che implementa una politica decisionale.
			\paragraph{CPU burst} è l'intervallo di tempo in cui viene usata intensamente la CPU. 
			\paragraph{I/O burst} è l'intervallo di tempo in cui viene usato intensamente I/O.
			\paragraph{CPU bound} processi con CPU burst lunghi, ad esempio compilatori, simulatori, calcolo del tempo, ecc\dots
			\paragraph{I/O Bound} processi con I/O burst lunghi, ciò comporta maggiore interattività con l'utente.
			\paragraph{Stato di IDLE} è lo stato in cui è una risorsa accesa e funzionante ma non utilizzata. \\	
							
			Un processo in esecuzione si trova o in CPU burst o in I/O burst. 			
			Lo scheduler, per essere efficiente, deve ottimizzare l'uso delle risorse in modo tale che, se la CPU è occupata con l'esecuzione di un processo, i dispositivi di I/O lo sono con un altro e viceversa. L'ottimizzazione della CPU viene dunque portata mediante lo scheduler. Per valutare la bontà di un algoritmo di scheduling si devono introdurre delle metriche di valutazione.
			$$T_{turnaround} = T_{termine} - T_{arrivo}$$
			$$T_{response} = T_{first-exec} - T_{arrivo}$$
			$$T_{wait} = T_{turnaround} - T_{job}$$
				
			\subsubsection{Algoritmo FIFO} 
				L'algoritmo FIFO (\textit{First In First Out}) mette in esecuzione il primo processo arrivato. Il problema a cui può portare questo algoritmo è l'\textbf{effetto convoglio}, ovvero quando un certo numero di piccoli consumatori di una risorsa vengono messi in coda dietro un enorme consumatore. 
				
			\subsubsection{Algoritmo SJF}
				L'algoritmo SJF (\textit{Shortest Job First}) mette in esecuzione il processo con CPU burst minore. In questo modo si evita l'effetto convoglio, ma solo se i processi arrivano allo stesso istante.
					
			\subsubsection{Algoritmo STCF}
				L'algoritmo STCF (\textit{Shortest Time to Completion First}) ogni volta che arriva un processo, lo compara il processo in esecuzione e lascia il processore a quello che ha CPU burst minore.\\
				
			SJF e STCF funzionano molto male per quanto riguarda il tempo di risposta (spesso possono indurre anche al verificarsi della starvation). Inoltre non conoscono a priori il CPU burst di un processo, perciò sono solo algoritmi teorici.
				
			\subsubsection{Round Robin}
				L'algoritmo \textbf{Round Robin} assegna un \textbf{quanto di tempo} ad ogni processo. Viene inizializzato un timer che, una volta arrivato a zero, forza un context switch. Il quanto di tempo va scelto bene, altrimenti si hanno troppi context switch se è troppo piccolo, o degenera in FIFO se è troppo grande.
			
			
		\subsection{Multilevel feedback scheduler}
			Il problema che \textbf{MLFQ} (\textit{MultiLevel Feedback Queue}) cerca di risolvere è: 
			\begin{itemize}
				\item Ottimizzare il $T_{turnaround}$.
				\item Aumentare l'interattività utente/sistema, minimizzando il $T_{response}$.
			\end{itemize}
			L'approccio che si usa consiste nell'avere un certo numero di \textbf{code} distinte, ognuna assegnata ad un diverso \textbf{livello di priorità}. MLFQ sfrutta i diversi livelli di priorità per decidere quale processo eseguire: viene scelto quello all'interno della coda di priorità maggiore. Se ci sono più processi all'interno di una certa coda, viene usato \textbf{RR}.
			\begin{itemize}
				\item 1. If priority(A) $>$ priority(B), A runs (B doesn't).
				\item 2. If priority(A) = priority(B), A \& B run in RR.
				\item 3. Quando un processo entra nel sistema, viene posizionato nella coda di priorità massima.
				\item 4a. Se un processo utilizza tutto il lasso di tempo a disposizione durante l'esecuzione, la sua priorità viene ridotta.
				\item 4b. Se un processo libera la CPU prima di terminare il lasso di tempo a disposizione, il livello di priorità rimane invariato.
				\item 5. \textbf{Priority boost }Dopo un certo periodo di tempo, tutti i processi vengono spostati nella coda di priorità più alta. (\textit{Evita la starvation dei long running jobs e il monopolio della CPU se qualche processo la rilascia poco prima del lasso di tempo.})
			\end{itemize}
			
			\subsubsection{Better accounting}
				La  scelta del tempo è cruciale, se settato troppo grande, i long running jobs potrebbero ancora andare in starvation, se impostato troppo piccolo, i processi interattivi potrebbero non avere una porzione adeguata dela CPU. 
				Per evitare che possa essere raggirato l'algoritmo di scheduling, lo scheduler tiene tracia di quanto tempo ha consumato un processo in un certo livello di \textbf{MLFQ}. Le regole 4a e 4b diventano: \\
				\begin{itemize}
					\item 4. Una volta che un processo ha usato il tempo a disposizione in un certo livello (indipendentemente da quante volte ha rilasciato la CPU), la sua priorità viene ridotta.
				\end{itemize}								

		
		\subsection{Address space}
			Un programma per essere eseguito deve risiedere in memoria. Essa può essere usata implicitamente (\textbf{stack:} \texttt{int x}) o esplicitamente (\textbf{heap:} \texttt{int *x = malloc(sizeof(int))}). Lo stack è gestito autonomamente, lo heap è gestito dal programmatore attraverso opportune funzioni (\texttt{malloc, realloc, free, \dots}), per cui non si conosce a priori la dimensione.
			
			\subsubsection{Memory API}
				\begin{itemize}
					\item \texttt{malloc()} Riceve in input un argomento di tipo \texttt{size\_t} (numero di bytes), se ha successo restituisce un puntatore all'inizio della zona allocata nello \textbf{heap}, se fallisce restituisce NULL.
					\item \texttt{free()} Riceve in input un puntatore, la grandezza della regione da liberare viene tenuta nella libreria \texttt{memoryallocation}.
				\end{itemize}
				
				La system call per la gestione diretta della memoria è \texttt{int brk(void *addr)}
				
			\subsubsection{Memory errors}
				\begin{itemize}
					\item \textbf{Dimenticarsi di allocare la memoria.} È da patchare il fatto che si possa indurre un \texttt{segfault} in modo tale da poter accedere al core dump della memoria e vedere dati sensibili (\textbf{attacchi core-dump}). È necessario eliminare questi dati dopo il loro utilizzo. 
					\item \textbf{Non allocare abbastanza memoria.} Può portare a vulnerabilità come il \textbf{buffer overflow}.
					\item \textbf{Dimenticarsi di inizializzare memoria allocata.} Potrebbero esserci valori come 0 o valori random.
					\item \textbf{Dimenticarsi di liberare la memoria.} Il \textbf{memory leak} può portare ad un esaurimento della memoria disponibile.
					\item \textbf{Liberare la memoria prima di aver finito di usarla.} Questo errore è chiamato \textbf{dangling pointer}, può causare un crash o la sovrascrittura di memoria valida.
					\item \textbf{Liberare la memoria più di una volta.} Problema noto come \textbf{double free}, il risultato è indefinito, la libreria \texttt{memory-allocation} potrebbe confondersi e fare cose strane. I crash sono la cosa più comune.
					\item \textbf{Chiamata di \texttt{free()} incorretta.} La funzione si aspetta un puntatore prodotto in precedenza da una \texttt{malloc()}. Quando viene passato alla \texttt{free }un valore diverso, possono succedere cose brutte e pericolose.
				\end{itemize}
				
			\subsubsection{Virtualizzazione della memoria}
				Con l'avvento della \textbf{multiprogrammazione} la memoria diviene una risorsa condivisa, bisogna iniziare a far fronte a tutte le problematiche che ciò comporta.
				\begin{itemize}
					\item \textbf{Protezione} un processo non può invadere lo spazio di un altro.
					\item \textbf{Interattività} Ci devono essere molti processi in esecuzione.								
				\end{itemize}
				Il meccanismo di astrazione che si vuole implementare prende il nome di \textbf{address space}, esso è il punto di vista di un processo sulla memoria del sistema, ovvero l'astrazione che il sistema operativo gli fornisce.
				
				Gli obiettivi della virtualizzazione della memoria sono riassunti come segue:
				\begin{itemize}
					\item \textbf{Trasparenza.} Il programmatore scrive il codice indipendentemente dalla grandezza della memoria.
					\item \textbf{Efficienza.} Il meccanismo di virtualizzazione non deve avere overhead troppo elevato.
					\item \textbf{Protezione.} Bisogna proteggere i processi da altri processi, dal sistema operativo, e viceversa.
				\end{itemize}
				
			\subsubsection{Mapping}
				Il \textbf{mapping} consiste nel trovare una corrispondenza fra indirizzo logico e indirizzo fisico. Nei sistemi \textbf{monoprogrammati} ciò era facile poichè ogni programma veniva mappato a partire dall'indirizzo \texttt{64KB} fino alla fine. Il compilatore assegnava ai programmi indirizzi costanti. Nel caso della \textbf{multiprogrammazione} invece, il compilatore assegna indirizzi preliminarli al programma, i quali  vengono successivamente rilocati. 
			
			\subsubsection{Base e Bound}
				Questa tecnica di mapping utilizza due registri, \textbf{base} e \textbf{bound}.
				Assunzioni: 
				\begin{itemize}
					\item Il programma viene caricato in locazioni contigue di memoria. (Un programma da \texttt{32KB} verrà caricato in \texttt{32KB} locazioni adiacenti)
					\item L'indirizzo logico è sempre minore dell'indirizzo fisico.
				\end{itemize}
				Mediante la rilocazione siamo in grado di calcolare l'indirizzo fisico come segue: 
				$$\texttt{indirizzo fisico = indirizzo logico + Base}$$
				\textbf{Base} è un registro contenente il punto di partenza (indirizzo fisico) del programma. \textbf{Bound} è il registro limite. Se un processo prova a saltare in zone di un altro processo viene generato un errore di segmentazione.
				
			\subsubsection{MMU}
				MMU sta per \textbf{Memory Managment Unit} ed è una componente hardware per la rilocazione degli indirizzi. L'input è un indirizzo logico prodotto dalla CPU, l'output è l'indirizzo fisico. Generalmente questa traduzione viene fatta a runtime. Prima di eseguire l'istruzione a cui sto puntando, l'indirizzo logico viene tradotto in indirizzo fisico (\textbf{rilocazione dinamica}).\\
				Ore che abbiamo la rilocazione dinamica, il sistema operativo deve fare le seguenti cose per implementare la memoria virtuale: 
				\begin{itemize}
					\item Quando un nuovo processo viene creato, il sistema operativo dovrà cercare in una struttura dati (spesso chiamata \textbf{free list}) spazio libero per il nuovo address space e marcarlo come in uso.
					\item Quando un processo termina, deve riabilitare tutta la memoria allocata per il processo all'interno della free list e pulire ogni struttura dati associata ad esso.
					\item Quando avviene un context switch deve salvare nel PCB i registri base e bound e ripristinare quelli del nuovo processo. 
					\item Quando un processo viene fermato è possibile muovere un address space da una locazione di memoria a un'altra. Basta deschedularlo, copiare l'address space dalla locazione corrente a quella nuova e infine aggiornare il registro \textbf{base}.
				\end{itemize}
				
				Il sistema operativo deve fornire degli \textbf{exception handler}. Per esempio, se un processo prova ad accedere a memoria al di fuori del suo \textbf{bound}, la CPU deve sollevare un'eccezione. 
				
		\subsection{Segmentazione}
			
			\subsubsection{Binding}
				
				Durante il processo di rilocazione vengono cambiati tutti gli indirizzi del programma per evitare che vadano fuori dallo spazio di indirizzamento previsto. Il \textbf{binding} è l'operazione che viene fatta per modificare gli indirizzi. Può essere: 
				\begin{itemize}
					\item \textbf{Early binding.} Rilocazione degli indirizzi fatta a \textbf{compile time}. Il compilatore deve conoscere la posizione di partenza del programma in memoria, ma funziona solo quando il compilatore genera direttamente il codice assoluto (\textit{sistemi embedded, monoprogrammati, \dots}).
					\item \textbf{Delayed binding.} La rilocazione degli indirizzi viene fatta durante il trasferimento del programma da disco a memoria (\textit{operazione svolta dal sistema operativo prima dell'introduzione dell'MMU}).
					\item \textbf{Late binding.} La rilocazione degli indirizzi viene fatta immediatamente prima di eseguire l'istruzione corrente, quindi a \textbf{runtime}. Per implementare questa tecnica serva l'MMU. 
				\end{itemize}
				
			\subsubsection{Segmentazione}
				
				Con la tecnica base e bound, c'è dello spazio potenzialmente non utilizzato tra lo stack e lo heap. L'idea alla base della \textbf{segmentazione} è quella di dividere il programma in \textbf{segmenti} che possono essere caricati in porzioni di memoria differenti siccome ad ognuno di essi è associata una coppia base-bound. I segmenti sono inseriti in modo indipendente all'interno della memoria fisica, in questo modo siamo in grado di evitare gli sprechi. Questo risparmio di memoria, tuttavia, complica notevolmente l'MMU, la quale deve gestire più segmenti presenti all'interno della memoria (ogni processo ha tre segmenti).\\
				Il meccanismo funziona come segue:
				\begin{itemize}
					\item \textbf{Input:} indirizzo logico $B$ (prodotto dal compilatore).
					\item Individua il segmento $s$ di appartenenza dell'indirizzo $B$.
					\item Calcola l'offset $k$ sottraendo all'indirizzo virtuale l'indirizzo di partenza (logico) del segmento ($k = B$ - indirizzo iniziale di $s$)
					\item Viene calcolato l'indirizzo fisico sommando $k$ e il base register (Indirizzo fisico = $Base(s)+k$)
				\end{itemize}
				Se un processo cerca di produrre un indirizzo illegale, l'hardware rileverà che l'indirizzo è out of bounds, trap nel sistema operativo, il quale terminerà il processo (\textbf{segmentation fault}).\\
				L'hardware per conoscere il segmento e l'offset taglia l'address space in segmenti basati sui primi bit dell'indirizzo virtuale (\textbf{approccio esplicito}). Nell'\textbf{approccio implicito} invece l'hardware determina il segmento in base a come è formato l'indirizzo. Se, ad esempio, l'indirizzo è stato generato dal program counter, appartiene al code segment; se è dello stack o del base pointer, deve appartenere al segmento stack. Ogni altro indirizzo viene interpretato come parte del segmento heap.
			
			\subsubsection{Stack}
				Siccome lo stack cresce al contrario, invece dei soli valori base e bound, l'hardware ha bisogno di sapere in quale direzione cresce il segmento (un bit settato a 1 se il segmento cresce positivamento, 0 negativamente). Il controllo del bound register viene fatto in valore assoluto.
				
			\subsubsection{Permessi}
				\paragraph{Code sharing. }Per risparmiare memoria, a volte è  utile condividere certi segmenti tra gli address spaces. Per supportare la condivisione abbiamo bisogno di \textbf{protection bits} da parte dell'hardware. Vengono aggiunti solamente pochi bit per segmento, a indicare quando un programma può leggerne, scriverne o eseguirne il codice contenuto. 
				
			\subsubsection{Coarse grained and fine grained}
				Gli esempi visti fin'ora utilizzavano la tecnica \textbf{coarse grained} (poche fette relativamente grandi). Alcuni dei primi sistemi erano più flessibili e permettevano che gli address spaces consistessero in un gran numero di piccoli segmenti, questo concetto era espresso come segmentazione \textbf{fine grained}. Ciò richiede un ulteriore supporto hardware, una \textbf{segment table} all'interno della memoria.
				
			\subsubsection{Frammentazione} 
				La segmentazione solleva un numero di nuove problematiche:
				\begin{itemize}
					\item Cosa dovrebbe fare il sistema operativo a fronte di un context switch? I segment registers devono essere salvati e ripristinati. 
					\item Come viene gestito lo spazio libero in memoria fisica? Quando un nuovo address space viene creato, il sistema operativo deve essere in grado di trovare lo spazio in memoria fisica per i suoi segmenti. 	
				\end{itemize}				 
				Il problema generale è che la memoria fisica consuma velocemente piccoli spazi liberi, rendendo difficile l'allocazione di nuovi segmenti o la crescita di quelli già esistenti. Questo problema è noto come \textbf{frammentazione esterna}. Si può risolvere con la \textbf{deframmentazione}, compattando la memoria fisica e riarrangiando i segmenti esistenti, copiando i dati dei segmenti in una regione contigua di memoria e cambiando il valore dei loro segment registers. Questa operazione è piuttosto complessa e dispendiosa oltre che bloccante (non si possono eseguire processi in quanto il loro indirizzo sta cambiando). Un approccio più semplice è quello di usare un algoritmo per la gestione della \textbf{free-list} che tenta di mantenere un elevato spazio disponibile contiguo in memoria. Purtroppo però la frammentazione esisterà sempre a prescindere da quanto buono sia l'algoritmo per minimizzarla.
				
		\subsection{Paginazione}
			La \textbf{paginazione} nasce per gestire in modo ottimale lo spazio libero in memoria e l'address space di un programma.
			Consiste nel tagliare gli spazi in fette di una certa dimensione. Anzichè dividere l'address space di un processo in segmenti, esso viene diviso in unità di dimensione fissata, ognuna delle quali è chiamata pagina.\\
			Vediamo la memoria fisica come un array di slots di dimensione fissata, chiamati \textbf{page frames}. Ogni frame può contenere una singola pagina di memoria virtuale. Ciò porta ad alcuni vantaggi: 
			\begin{itemize}
				\item \textbf{Flessibilità.} Il sistema sarà in grado di supportare l'astrazione dell'address space efficacemente, a prescindere da come un processo ne fa uso. Non vogliamo, ad esempio, dover fare assunzioni riguardo la direzione di crescita dello heap e dello stack e come vengono usati.
				\item \textbf{Semplicità} della gestione dello spazio libero. Per esempio, supponiamo che il sistema operativo desideri  inserire il nostro addess space da \texttt{64B} in memoria fisica. Siccome i programmi sono divisi in pagine di dimensione fissata, il problema della segmentazione viene ridotto di molto visto che, siccome il sistema operativo tiene traccia della free list, gli basta semplicemente prendere il primo frame disponibile e assegnarlo a una pagina.
			\end{itemize}
			Per memorizzare dove ogni pagina virtuale dell'address space è posizionata in memoria fisica, il sistema operativo tiene una struttura dati per  ciascuno processo nota come \textbf{page table}. Il ruolo principale della page table è di memorizzare, per ogni pagina virtuale dell'address space, il corrispondente frame fisico.	
			
			\subsubsection{Address translation}
				Per tradurre l'indirizzo virtuale generato da un processo, dobbiamo per prima cosa dividerlo in \textbf{Virtual Page Number (VPN)} e \textbf{offset}.
				Siccome si conosce la dimensione di ciascuna pagina, si può dividere l'indirizzo virtuale in:
				\begin{itemize}
					\item \textbf{VPN:} Bit più significativi che fanno da indice per accedere alla page table del processo per trovare il frame fisico corrispondente (\textbf{PFN}).
					\item \textbf{Offset:} Bit che servono per indirizzare la grandezza di una pagina.
				\end{itemize}
				A questo punto si traduce l'indirizzo virtuale in fisico sostituendo il \textbf{Physical Frame Number (PFN)} al VPN.
			
			\subsubsection{Page tables}
				Le page tables possono essere terribilmente grandi. Per esempio, immaginiamo un address space da \texttt{32 bit} con pagine da \texttt{4KB}. L'indirizzo virtuale sarà diviso in \texttt{20 bit} di VPN e \texttt{12 bit} di offset. \texttt{20 bit} di VPN implicano $2^{20}$ possibili traduzioni per ogni processo. Assumendo di aver bisogno di \texttt{4B} per \textbf{page table entry (PTE)} per mantenere la traduzione fisica più ogni altra informazione utile otteniamo \texttt{4MB} di memoria necessari per ogni page table. Con 100 processi in esecuzione, questo significa che il sistema operativo avrà bisogno di \textbf{400MB} di memoria. 
				\paragraph{Cosa contiene una page table?} La page table è sempicemente una struttura dati usata per mappare gli indirizzi virtuali in indirizzi fisici. La forma più semplice è chiamata \textbf{page table lineare} che è semplicemente un array. Il sistema operativo indicizza l'array con il VPN e consulta la PTE a quell'indice per trovare il PFN desiderato.\\ 
				Ogni PTE contiene diversi bit:
				\begin{itemize}
					\item \textbf{Valid bit.} Indica quando una particolare traduzione è valida. Per esempio, quando un programma inizia l'esecuzione, avrà code e heap a un'estremità del suo spazio di indirizzamento e lo stack dall'altra. Tutto lo spazio non utilizzato in mezzo sarà marcato come invalido e se il processo tenterà di accedervi, verrà generata una trap al sistema operativo che lo terminerà. È cruciale per supportare un address space sparso. 
					\item \textbf{Protection bits.} Indicano quando una pagina può essere letta, scritta o eseguita. Accedere a una pagina in modo non consentito da questi bit genererà una trap nel sistema operativo, il quale terminerà il processo.
					\item \textbf{Present bit.} Indica se la pagina in questione è in memoria fisica o su disco. Consente al sistema operativo di swappare le pagine liberando la memoria fisica.
					\item \textbf{Dirty bit.} Indica se la pagina è stata modificata da quando risiede in memoria.
					\item \textbf{Reference bit.} Viene usato per tenere traccia se una pagina è stata acceduta da quando risiede in memoria. 
				\end{itemize}
			
			\subsubsection{Quanto è lenta la paginazione?}
				Per ogni riferimento a memoria (sia per prelevare un'istruzione che per un load o store esplicito), la paginazione ne necessita uno aggiuntivo per prelevare la traduzione dalla page table. I riferimenti a memoria aggiuntivi sono costosi e in questo caso rallenteranno il processo di un fattore pari a due o più.
				
		\subsection{Translation Lookaside Buffer}
			Siccome le informazioni di mappatura risiedono generalmente in memoria fisica, la paginazione richiede un accesso aggiuntivo per ogni indirizzo virtuale generato dal programma. L'obbiettivo è snellire la tecnica introdotta, cercando di \textbf{diminuire il numero di accessi a memoria fisica }(alla page table). Viene aggiunta alla MMU una cache hardware delle traduzioni virtual-to-physical più popolari chiamata \textbf{translation lookaside buffer  o TLB}. Per ogni indirizzo virtuale, l'hardware controlla per prima cosa il TLB per vedere se la traduzione desiderata è presente al suo interno. 
			\begin{lstlisting}[style=CStyle]
VPN = (VirtualAddress & VPN_MASK) >> SHIFT
(Success, TlbEntry) = TLB_Lookup(VPN);
if (Success == True){	//TLB HIT
	if (CanAccess(TlbEntry.ProtectBits == True){
		Offset = VirtualAddress & OFFSET_MASK;
		PhysAddr = (TlbEntry.PFN << SHIFT) | Offset;
		Register = AccessMemory(PhysAddr);				
	}
	else
		RaiseException(PROTECTION_FAULT);
}	
else{						//TLB MISS
	PTEAddr = PTBR + (VPN * sizeof(PTE));	
	PTE = AccessMemory(PTEAddr);
	if(PTE.Valid == False)
		RaiseException(SEGMENTATION_FAULT);
	else if (CanAccess(PTE.ProtectBits) == False)
		RaiseException(PROTECTION_FAULT);
		else{
			TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits);
			RetryInstruction();
		}								
}			\end{lstlisting}
			L'algoritmo che l'hardware segue funziona in questo modo: 
			\begin{itemize}
				\item Estrae il VPN dall'indirizzo virtuale.
				\item Controlla se il TLB contiene la traduzione per il VPN. Se così fosse, abbiamo un \textbf{TLB hit}, la traduzione è cioè contenuta in cache.
				\item Se la CPU non trova la traduzione nella TLB abbiamo un \textbf{TLB miss}. L'hardware accede alla page table per trovare la traduzione e, assumendo che l'indirizzo virtuale generato dal processo sia valido e accessibile, aggiorna il contenuto del TLB con la nuova entry. Queste operazioni sono parecchio costose.
				\item Una volta che il TLB è aggiornato, l'hardware riprova l'istruzione, ottenendo un TLB hit.
			\end{itemize}
			
			\subsubsection{Performance  e località}
				Il TLB migliora le performance grazie al \textbf{principio di località}. Esso si divide in:
				\begin{itemize}
					\item \textbf{Spaziale.} Se la CPU sta eseguendo un'istruzione presente in memoria, vuol dire che con molta probabilità le prossime istruzioni da eseguire si troveranno fisicamente nelle vicinanze di quella in corso.
					\item \textbf{Temporale.} Se accedo all'istruzione 100 al tempo $t_0$, con molta probabilità acederò nuovamente ad essa negli istanti di tempo successivi. 
				\end{itemize}
			
			\subsubsection{TLB miss}
				Chi gestisce un TLB miss? Ci sono due possibili risposte:
				\begin{itemize}
					\item \textbf{Hardware.} L'HW deve sapere la posizione delle page tables in memoria (attraverso il page table register), oltre al loro formato esatto. In presenza di un miss, l'HW deve accedere alla page table, trovare la PTE corretta, estrarre la traduzione desiderata, aggiornare il TLB con la pagina contenente l'indirizzo fisico ricercato e riprovare l'istruzione.
					\item\textbf{Software (S.O).} Al verificarsi di un TLB miss, l'hardware solleva un eccezione per mettere in pausa il flusso corrente di istruzioni, aumenta i privilegi a livello kernel e salta a un trap handler. Questo trap handler è codice scritto all'interno del sistema operativo, il cui scopo è la gestione esplicita dei TLB misses. Il codice cercherà la traduzione nella page table, userà "speciali" istruzioni privilegiate per aggiornare il TLB e, infinie, eseguirà la \textbf{return-from-trap}. A questo punto, l'hardware riproverà l'istruzione (TLB hit).
				\end{itemize}
				\paragraph{TLB return from trap} In questo caso, quando si torna da una TLB miss-handling trap, l'hardware deve ripristinare l'esecuzione dall'istruzione che aveva causato la trap nel sistema operativo.
				
				Quando il TLB miss-handler è in esecuzione, il sistema operativo deve essere molto attento a non causare una catena infinita di TLB misses. Se ho un miss, viene generata un'eccezione. Bisogna fare un context switch per permettere al S.O. di gestire l'evento. Per mandarlo n esecuzione bisogna mettere l'indirizzo del TLB miss-handler nel PC. Questo indirizzo tuttavia, come tutti gli altri, viene passato all'MMU. Quest'ultima lo cerca nel TLB, ottenenedo un miss. Parte quindi un loop. La soluzione che viene adottata per risolvere questo problema consiste nel tenere il miss handler all'interno del TLB.
				
				
			\subsubsection{TLB - contenuto}
				Una address-translaion cache tipica potrebbe avere 32, 64 o 128 entries ed essere ciò che viene chiamato \textbf{fully associative}. Ciò significa che una traduzione potrebbe essere ovunque nel TLB e l'hardware dovrà cercare in parallelo fino a trovare la traduzione desiderata. Una entry del TLB ha il seguento aspetto: \texttt{VPN | PFN | other bits}.\\
				Tra gli other bits generalmente ci sono il \textbf{valid bit}, i \textbf{protection bits}, \dots
				
			\subsubsection{TLB - Context Switch}
				Il TLB contiene traduzioni virtual to physical che sono valide per il processo in esecuzione ma prive di significato per gli altri. Bisogna assicurarsi che quando cambiamo processo, il processo che sta per essere eseguito non usi le traduzioni di quello precedente. Un approccio semplice ma inefficace è fare un \textbf{flush} (impostando tutti i valid bit a 0) del TLB a fronte di un context switch. Ogni volta che un processo verrà eseguito, incapperà in TLB misses.\\
				Per ridurre questo overhead, alcuni sistemi aggiungono un supporto hardware per abilitare la condivisione del TLB attraberso context switcher. In particolare alcuni sistemi hardware forniscono un campo \textbf{Address Space Identifier} (ASID) nel TLB. 
				
		\subsection{Multi Level Page Tables}		
			Le page tables sono grandi e consumano troppa memoria. 
			
			\subsubsection{Bigger pages}
				Una possibile soluzione è quella di fare pagine più grandi. Il problema è che questo comporta a sprechi di spazio all'interno delle pagine stesse (\textbf{frammentazione interna}). La memoria si riempie subito di pagine contenenti parecchio spazio vuoto.
			
			\subsubsection{Paginazione e segmentazione}
				Assumiamo di avere un address space nel quale la porzione usata da stack e heap è piccola. Per esempio, usiamo uno spazio di indirizzamento da \texttt{16KB} con pagine da \texttt{1KB}. La page table relativa a questo address space sarà quindi:
				\begin{center} \begin{tabular}{|c|c|c|c|c|}
					\hline					
					PFN & valid & prot & present & dirty\\
					\hline
					10 & 1 & r-x & 1 & 0\\
					- & 0 & \_ & - & - \\
					- & 0 & \_ & - & - \\
					- & 0 & \_ & - & - \\
					23 & 1 & rw- & 1 & 1\\
					- & 0 & \_ & - & - \\
					- & 0 & \_ & - & - \\
					- & 0 & \_ & - & - \\
					- & 0 & \_ & - & - \\
					- & 0 & \_ & - & - \\
					- & 0 & \_ & - & - \\
					- & 0 & \_ & - & - \\
					- & 0 & \_ & - & - \\
					- & 0 & \_ & - & - \\
					28 & 1 & rw- & 1 & 1\\
					4 & 1 & rw- & 1 & 1\\
					\hline
				\end{tabular} \end{center}
				Come possiamo osservare dalla figura, la maggior parte della page table non è utilizzata.
				Invece di avere una singola page table per l'intero address space del processo, perchè non averne una per segmento logico? Con la segmentazione avevamo un registro \textit{base} che ci diceva dove ogni segmento risiedeva in memoria fisica e un registro \textit{bound} che ne esprimeva la grandezza. Nel nostro approccio ibrido, abbiamo queste strutture nell'MMU; qui, non usiamo il \textit{base} per puntare al segmento stesso, ma teniamo l'indirizzo fisico della page table di quel segmento. Il registro \textit{bound} è usato per indicare la fine della page table relativa a un segmento.\\
				Nell'hardware, assumiamo che ci siano tre paia di \textit{base/bound}: una per code, heap e stack. In un context switch, questi registri devono essere cambiati per riflettere la locazione delle page tables del nuovo processo in esecuzione. In un TLB miss, l'hardware usa i bits del segmento per determinare quale coppia \textit{base/bound} usare. L'hardware quindi prende il \textit{base} corretto e lo combina col VPN come segue, per formare l'indirizzo della PTE: 
				\begin{lstlisting}[style=CStyle]
SN = (VirtualAddress & SEG_MASK) >> SN_SHIFT
VPN = (VirtualAddress & VPN_MASK) >> VPN_SHIFT
AddressOfPTE = Base[SN] + (VPN * sizeof(PTE)) \end{lstlisting}
				La differenza critica, sta nella presenza di un registro \textit{bound} per segmento. Ogni \textit{bound} contiene il valore della massima pagna valida nel segmento. Se il code segment sta usando le sue prime tre pagine (0, 1 e 2), la page table del segmento avrà solamente tre entries allocate e il bound register sarà impostato a 3. In questa maniera, il nostro approccio ibrido realizza un risparmio di memoria significativo rispetto alla classica page table lineare.\\ 
				
				Purtroppo questo approccio si porta dietro con sè tutti i problemi della segmentazione, e se avessimo ad esempio heap molto grandi e sparsi in memoria, finiremmo con l'avere ancora una volta sprechi per via dele page table. Seconda cosa, il manifestarsi ancora una volta della frammentazione esterna. 
				
			\subsubsection{Multi Level Page Tables}
				Un altro approccio potrebbe essere trasformare una page table lineare in qualcosa di sime a un albero.
				\begin{itemize}
					\item Per prima cosa, viene tagliata la page table in unità page-sized.
					\item Se una pagina di una PTE è invalida, non viene allocata.
					\item Per tenere traccia se una pagina delle page table è valida (e se valida, dove risiede in memoria), usiamo una nuova struttura chiamata \textbf{page directory}.
				\end{itemize}
				Ciò che fa la \textbf{multi-level table} è far scomparire parti della page table lineare e tenere traccia di quali pagine sono allocate.\\
				
				La page directory, consiste in una serie di \textbf{page directory entries (PDE)}, le quali hanno un valid bit e un numero di page frame (\textbf{PFN} - in questo caso rappresenta l'indirizzo di memoria dove è situata una page table). Il bit di valità è un po' diverso, se la PDE è valida significa che almeno una delle pagine della page table a cui la entry punta (via PFN) è valida. 
				\paragraph{Vantaggi:}
				\begin{itemize}
					\item La multi-level table alloca spazio \textbf{solamente per la page table in proporzione all'ammontare di address space in uso} (\textit{se c'è tanto spazio in mezzo tra due pagine utilizzate, vengono comunque allocate solo due pagine}). 
					\item Se implementata correttamente, \textbf{ogni porzione della page table entra ordinatamente in una pagina}, rendendo più facile la gestione della memoria; il sistema operativo prende semplicemente la prossima pagina libera quando ha bisogno di allocare o far crescere una page table.
				\end{itemize}
				Abbiamo aggiunto l'\textbf{indirezione}, che ci permette di posizionare pagine della page table ovunque vogliamo in memoria fisica. Questa tecnica ha un costo: in un TLB miss saranno necessari due caricamenti da memoria per prelevare la traduzione corretta dalla page table (una per la page directory e una per la PTE stessa, \textit{trade off time-space}). Nel caso medio (TLB hit), le performance sono \textbf{identiche }alla page table lineare. Un altro aspetto negativo è la \textbf{complessità}, che sia l'hardware o il sistema operativo a gestire la consultazione delle page tables.
			
			\subsubsection{Più di due pagine}
				Bisogna evitare che la page directory diventi troppo grande, altrimenti l'obiettivo di fare in modo che ogni pezzo della multi-level page table entri in una pagina svanisce. Quando si ha a che fare con pagine piuttosto piccole che lasciano parecchi bit di VPN, è preferibile splittare la page directory stessa in più pagine, aggiungendo un'altra page directory sopra ad essa. 
				
		\subsection{Page Fault e Swap}
			Per supportare address spaces di grandi dimensioni (per permettere ai processi di non preoccuparsi se c'è abbastanza spazio in memoria), il sistema operativo avrà bisogno di posizionare altrove le pagine che non sono largamente richieste.
			
			\subsubsection{Swap space}
				La prima cosa da fare è riservare un po' di spazio su disco per muovere le pagine avanti e indietro. Nei sistemi operativi, ci riferiamo a questa locazione come \textbf{swap space}. La sua dimensione è importante, in quanto determina il numero massimo di pagine di memoria che possono essere usate da un sistema ad un dato istante di tempo.
				
			\subsubsection{Present bit}
				Quando l'hardware guarda nella PTE, potrebbe scoprire che la pagina non è presente in memoria fisica. Il modo in cui l'hardware  (o il sistema operativo in caso di software-managed TLB) determina ciò è attraverso un nuovo \textbf{present bit} in ogni PTE. Se il present bit è a 0, la pagina è da qualche parte su disco. A fronte di un \textbf{page fault}, viene invocato il sistema operativo, il quale manda in esecuzione un \textbf{page-fault handler}.
				
			\subsubsection{Page Fault}
				Se una pagina non è presente, il sistema operativo viene messo al comando per gestire il page fault sia nei sistemi hardware-managed TLB che nei software-managed TLB. Il sistema operativo può usare i bits della PTE relativi al PFN come indirizzo su disco. Quando l'I/O del disco è completato, il sistema operativo aggiorna la page table per marchiare la pagina come presente e aggiorna il campo PFN della PTE e riprova l'istruzione. Questo nuovo tentativo potrebbe generare un TLB miss (è possibile aggiornare anche il TLB a seguito di un page fault per evitare questo scenario). Mentre viene fatto I/O il sistema operativo sarà libero di eseguire altri processi in ready. 
				
			\subsubsection{Memoria piena}
				Il sistema operativo potrebbe voler prima swappare una o più pagine su disco per fare spazio a quelle nuove in procinto di caricare. Il processo di scegliere una pagina da sostituire è noto come \textbf{page replacement policy}. Spostare la pagina sbagliata può avere dei costi elevati in termini di performance (\textit{si può causare una velocità disk-like, 10.000 o 100.000 volte più lento}).\\
				
				A fronte di un page fault, il sistema operativo deve trovare il frame fisico per far risiedere la pagina, e se tale frame non c'è, bisogna aspettare che l'algoritmo di replacement venga eseguito e liberi delle pagine dalla memoria rendendole disponibili per l'utilizzo.
			
			\subsubsection{Replacements}
				Piuttosto che aspettare che si riempa la memoria, il sistema operativo tiene un piccolo ammontare di memoria libera. In molti sistemi, vengono utilizzati un \textbf{high watermark} (HW) e un \textbf{low watermark} (LW) per facilitare la decisione di quando iniziare a sfrattare le pagine. Quando il sistema operativo nota che ci sono meno di LW pagine disponibili, un thread (\textbf{swap deamon}) in background rensponsabile della liberazione della memoria viene eseguito. Il thread sfratta le pagine fino a quando non ce ne sono HW disponibili. 
			
				
		\subsection{Replacement policies}
			Decidere quale pagina (o pagine) sfrattare è incapsulato all'interno della \textbf{politica di replacement} del sistema operativo.
			
			\subsubsection{Cache management}
				È possibile vedere il nostro obiettivo come la massimizzazione del numero di cache hits. Conoscere il numero di cache hits e misses ci permette di calcolare l'\textbf{average memory access time} (AMAT).
				$$AMAT = T_M + (P_{MISS}*T_D)$$
				Dove $T_M$ rappresenta il costo di accesso a memoria, $T_D$ il costo di accesso a disco, e $P_{MISS}$ la percentuale di miss (da 0.0 a 1.0). 
				
			\subsubsection{Optimal replacement policy}
				La politica ottimale di replacement conduce al minor numero di misses in generale. Se dobbiamo sfrattare delle pagine, perchè non selezionare quelle che verranno usate più avanti nel tempo? È un approccio semplice ma difficile da implementare.
				
			\subsubsection{FIFO policy}
				FIFO (\textit{first in first out}) ha un buon punto di forza: è semplice da implementare. Purtroppo non è in grado di determinare l'importanza dei blocchi, se una pagina viene acceduta parecchie volte, FIFO deciderà comunque di sfrattarla.
			
			\subsubsection{Random policy}
				L'algoritmo random, che sceglie una pagina casuale da sostituire, ha proprietà simili al FIFO, è semplice da implementare ma non sceglie intelligentemente i blocchi da sfrattare.
				
			\subsubsection{LFU and LRU }
				Per evitare di sfrattare pagine importanti sfruttiamo il \textbf{principio di località}. Se un processo accede una pagina di recente, è molto probabile che quest'ultima verrà acceduta nuovamente nel futuro prossimo. Un tipo di informazione "storica" che potrebbe essere usata in una politica di page replacement è la \textbf{frequenza}. La politica \textbf{Least Frequently Used} (LFU) sostituisce le pagine usate meno di frequente. Simile è LRU \textbf{Least Recently Used} che sostituisce la pagina usata meno di recente.\\		
				Esistono anche una classe di algoritmi opposti, Most Frequently Used MFU e Most Recently used MRU.
				
			\subsubsection{LRU approssimato}
				Dato che scansionare tutti i tempi per trovare la pagina least recently used è molto costoso, possiamo usare un'approssimazione. L'idea richiede supporto hardware, nella forma di \textbf{use bit}. Questo bit è contenuto in ogni pagina del sistema e ogni volta che una di esse viene riferita (letta o scritta), lo use bit è settato dall'hardware a 1. L'hardware non pulisce mai il bit, è il sistema operativo che ha il compito di settarlo a 0. Ci sono molti modi ma il \textbf{clock algorithm} è un approccio molto semplice e funzionale. Immaginiamo tutte le pagine del sistema arrangiate in una lista circolare. Una \textbf{clock hand} punta a una pagina (non importa quale). Quando deve essere fatta una sostituzione, il sistema operativo controlla se la pagina puntata P ha lo use bit a 1 o a 0. Se a 1 non è un buon candidato per la sostituzione, il bit viene settato a 0 e la clock hand passa alla prossima pagina. L'algoritmo continua fino a quando non trova una pagina con use bit a 0. Se la pagina è \textbf{dirty}, deve essere riscritta su disco prima di essere sfrattata, quindi molti sistemi preferisco pulire pagine \textbf{clean}.
				
			\subsubsection{Trashing}
				Cosa dovrebbe fare il sistema operativo quando la memoria è semplicemente sovraccaricata e la richiesta di memoria dell'insieme dei processi in esecuzione eccede la memoria fisica disponibile? In questi casi il sistema è in costante paginazione (\textbf{trashing}). 
				\begin{itemize}
					\item \textbf{Admission control.} Dato un insieme di processi, un sistema può decidere di non eseguirne un sottoinsieme.
					\item \textbf{Out of memory killer.} Quando la memoria è sovraccarica, questo demone sceglie un processo che sta usando intensamente la memoria e lo termina, riducendo piano piano l'utilizzo della risorsa. (\textit{Linux})
				\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%				
	\newpage	
	\section{Concurrency}
		\subsection{Threads e locks}
			Un \textbf{thread }è un sottoinsieme delle istruzioni di un processo, che può essere eseguito in maniera concorrente con altre parti di esso. L’obiettivo dei threads è quello di rendere più veloce l’esecuzione di un processo. Un programma multi-thread ha più punti di esecuzione (molteplici PCs, da ognuno dei quali vengono prelevate ed eseguite istruzioni). Possono essere visti come processi separati che \textbf{condividono lo stesso address space}. Ogni thread ha il proprio insieme privato di registri. Se ci sono due threads in esecuzione su un singolo processore, per switchare da T1 a T2 deve avvenire un context switch. Invece che il PCB avremo bisogno di un \textbf{Thread Control Blocks} (TCBs) per memorizzare lo stato di ogni thread di un processo. La differenza principale tra thread switch e context switch è che nel primo caso l'address space rimane lo stesso.\\
			Un altra grande differenza tra threads e processi riguarda lo stack. In un processo multi-thread, ogni thread è indipendente e potrebbe chiamare varie routines. Invece di un singolo stack nell'address space ce ne sarà uno per thread (\textbf{thread-local storage}).\\
			
			Utilizzare i thread abilita la sovrapposizione dell'I/O con altre attività all'interno di un singolo programma.
			
			\subsubsection{Thread creation}
				Vogliamo creare un programma che generi due threads. Ogni threads eseguirà la funzione \texttt{mythread()} con argomenti diversi (stringa A o B). Una volta che un thread viene creato, potrebbe venire eseguito subito (dipende dallo scheduler) o essere messo in stato di ready. 
				\begin{lstlisting}[style=CStyle]
#include <stdio.h>
#include <assert.h>
#include <pthread.h>

void *mythread(void *arg) {
	printf("%s\n", (char *) arg);
	return NULL;
}


int main(int argc, char *argv[]) {
	pthread_t p1, p2;
	int rc;
	printf("main: begin\n");
	rc = pthread_create(&p1, NULL, mythread, "A"); assert(rc==0);
	rc = pthread_create(&p2, NULL, mythread, "B"); assert(rc==0);
	// join waits for the threads to finish
	rc = pthread_join(p1, NULL); assert(rc==0);
	rc = pthread_join(p2, NULL); assert(rc==0);
	printf("main: end\n");
	return 0;
} 				\end{lstlisting}
				Il thread principale chiama \texttt{pthread\_join()} che aspetta il completamento di un particolare thread. L'ordine in cui viene eseguito il programma, dipende solo dallo scheduler.
			
			\subsubsection{Dati condivisi}
				La \textbf{race condition} consiste nell'avere più threads che concorrono all'uso della stessa risorsa. Il risultato della computazione \textbf{non è deterministico} in quanto dipende esclusivamente dalle decisioni dello scheduler e da quanto siamo fortunati con i timer interrupts. Una \textbf{sezione critica} è un pezzo di codice che accede alle variabili condivise e non deve essere eseguita simultaneamente da più thread. Abbiamo bisogno della \textbf{mutua esclusione}, la quale garantisce che, se un thread è in esecuzione in sezione critica, agli altri verrà proibilito l'accesso.
				
			\subsubsection{Atomicità}
				Un modo per risolvere il problema potrebbe essere quello di avere istruzioni più potenti che, in un singolo passo, facciano esattamente ciò di cui abbiamo bisogno. In questo caso è l'hardware a garantire l'atomicità, tramite delle \textbf{synchronization primitives}.
				
			\subsubsection{Thread creation}
				\begin{lstlisting}[style=CStyle]
#include <pthread.h>
int pthread_create( pthread_t *thread, const pthread_attr_t *attr, void * (*start_routine) (void*), void * arg ); \end{lstlisting}
				
				\begin{itemize}
					\item \texttt{pthread\_t *thread} è un puntatore a una struttura di tipo \texttt{pthread\_t} che useremo per interagire con i thread.
					\item \texttt{attr} viene usato per specificare ogni tipo di attributo che il thread potrebbe avere. (\textit{Grandezza stack, informazioni riguardanti priorità di scheduling, \dots})
					\item Il terzo argomento è un \textbf{puntatore a funzione} e consiste nel nome della funzione che vogliamo far eseguire al thread creato.
					\item \texttt{arg} è l’argomento che deve essere passato alla funzione che il thread deve eseguire.
				\end{itemize}
				
				I puntatori sono \texttt{void }perchè permettono di passare ogni tipo di argomento facendo semplicemente un cast.
		
			\subsubsection{Thread completion}
				Se vogliamo aspettare il completamento di un thread, dobbiamo chiamare la routine \texttt{pthread\_join()}
				\begin{lstlisting}[style=CStyle]
#include <pthread.h>
int pthread_join( pthread_t thread, void **value_ptr ); \end{lstlisting}
				\begin{itemize}
					\item \texttt{thread} è usato per specificare quale thread stiamo aspettando.
					\item \texttt{**value\_ptr} è un puntatore al valore di ritorno.
				\end{itemize}

				
		\subsection{Locks}
			I lock vengono usati per introdurre la \textbf{mutua esclusione}, permettendo quindi di eseguire atomicamente la sezione critica. Per usare un lock, basta aggiungere il codice necessario attorno alla sezione critica come segue:
			\begin{lstlisting}[style=CStyle]
lock_t mutex; //lock allocato globalmente
...
lock(&mutex);
x = x + 1; 		//sezione critica
unlock(&mutex); \end{lstlisting}				
			
			Un lock è una variabile, e come tale va \textbf{dichiarata e inizializzata}. Un lock, ad un certo istante di tempo, può trovarsi in due stati: \textbf{disponibile} o \textbf{acquisito}. Il funzionamento è questo:
			\begin{itemize}
				\item viene chiamata la routime \texttt{lock()} per acquisire il lock.
				\item Se disponibile, il thread chiamante riceve il lock e può entrare in sezione critica. Se acquisito, il thread chiamante rimarrà bloccato nella routine \texttt{lock()} fino a quando il thread in sezione critica non termina e invoca la \texttt{unlock()}.
				\item Una volta acquisito il lock, un thread può operare in sezione critica. 
			\end{itemize}
			Il nome della libreria \texttt{POSIX} per un lock è \textbf{mutex}. È possibile proteggere la sezione critica con un unico grande lock (\textbf{coarse-grained}) ma è possibile usare svariati lock (\textbf{fine-grained}). Mediante il lock, il programmatore guadagna un po' di \textbf{controllo sullo scheduler}. I locks però devono avere le seguenti proprietà:
			\begin{itemize}
				\item \textbf{Correctnes.} Garantire la mutua esclusione.
				\item \textbf{Fairness.} Evitare che i thread vadano in starvation. Tuttu devono poter accedere alla sezione critica prima o poi.
				\item \textbf{Performance.} L'overhead di time dovuto all'introduzione dei lock non deve minare le performance. 
			\end{itemize}
			Per progettare un lock funzionante, abbiamo bisogno di aiuto da parte dell'hardware e dal sistema operativo.
			\subsubsection{Controlling interrupts}
				Se gli interrupt vengono disabilitati prima di entrare in sezione critica e riabilitati dopo, abbiamo implementato un rudimentale lock.
				\begin{lstlisting}[style=CStyle]
void lock() { DisableInterrupts(); }
void unlock() { EnableInterrupts(); } \end{lstlisting}	
				
				\paragraph{Vantaggi}
					\begin{itemize}
						\item In intel \texttt{CLI} e \texttt{STI} sono già presenti nell'ISA del processore, non dobbiamo quindi apportare modifiche dal punto di vista hardware.
						\item Soluzione molto semplice.
					\end{itemize}

				\paragraph{Svantaggi}
					\begin{itemize}
						\item È necessario avere fiducia nei threads (\texttt{CLI} e \texttt{STI} sono istruzioni privilegiate). Un loop infinito in sezione critica potrebbe causare una catastrofe.
						\item Non funziona bene in presenza di più processori, visto che non siamo in grado di disabilitare gli interrupts su tutte le CPU.
						\item Disabilitare gli interrupts per periodi di tempo troppo estesi può portare alla perdita di alcuni di essi. Ad esempio, la CPU può perdersi il fatto che il disco ha comunicato di aver terminato la read request.
						\item Inefficienza.	
					\end{itemize}
			\subsubsection{Load e Store}
				\begin{lstlisting}[style=CStyle]
typedef struct __lock_t { int flag; } lock_t;

void init(lock_t *mutex) {
	// 0 -> lock is available, 1 -> held
	mutex->flag = 0;
}

void lock(lock_t *mutex) {
	while (mutex->flag == 1) // TEST the flag
		; // spin-wait (do nothing)
	mutex->flag = 1; // now SET it!
}

void unlock(lock_t *mutex) {
	mutex->flag = 0;
}				\end{lstlisting}
				Viene usata una flag per indicare se un thread è in possesso del lock. Il primo thread che entrerà in sezione critica chiamerà la routine \texttt{lock()}, la quale controllerà il valore di flag e la setterà a 1 nel caso in cui essa sia uguale a 0 (a indicare che il thread è ora in possesso del lock) o farà spin-lock in caso contrario. Una volta finito la sezione critica, il thread chiama la \texttt{unlock()} e pulisce flag (rilasciando il lock). Se un altro thread chiamasse la \texttt{lock()} mentre il primo è in sezione critica, esso farà \textbf{spin-wait} nel ciclo while fino a quando non verrà chiamata la \texttt{unlock()}. Ci sono però due problemi:
				\begin{itemize}
					\item \textbf{Correctness.} È possibile che entrambi i thread settino flag a 1 ed entrino in sezione critica. Se \texttt{T1} vede il lock a 0 e prima di settarlo si ha un interrput, sia \texttt{T2} che \texttt{T1} setteranno il flag a 1 ed entreranno in sezione critica.
					\item \textbf{Performance.} Questo ciclio è chiamato ciclio di busy waiting o \textbf{spinlock}. Il thread è in uno stato di attesa che mantiene occupato il processore, vengono quindi sprecati cicli di CPU
				\end{itemize}
				
			\subsubsection{Test and Set}
				Viene implementato il supporto hardware con l'istruzione \texttt{TestAndSet}:
				\begin{lstlisting}[style=CStyle]
int TestAndSet( int *old_ptr, int new) {
	int old = *old_ptr; //prelevo vecchio valore di ptr
	olt_ptr = new;			//inserisco "new" in old_ptr
	return old;					//restituisco il vecchio valore
}				\end{lstlisting}
				Restituisce il vecchio valore puntato da \texttt{ptr} e lo aggiorna a \texttt{new}. La chiave di questo meccanismo è che questa sequenza di operazioni viene eseguita \textbf{atomicamente}. 
				\begin{lstlisting}[style=CStyle]
typedef struct __lock_t {
	int flag;
} lock_t;

void init(lock_t *lock) {
	// 0 indicates that lock is available, 1 that it is held
	lock->flag = 0;
}

void lock(lock_t *lock) {
	while (TestAndSet(&lock->flag, 1) == 1)
		; // spin-wait (do nothing)
}

void unlock(lock_t *lock) {
	lock->flag = 0;
}				\end{lstlisting}
				Con questo tipo di lock, siamo sicuri che un singolo thread potrà acquisire il lock ed entrare in sezione critica. Per funzionare correttamente su un singolo processore, ha bisogno di un \textbf{preemptive scheduler} (situazione per cui un processo viene temporaneamente interrotto e portato fuori dalla CPU), ad esempio che interromperà un thread attraverso un timer. Senza, non ha senso siccome un thread in spinning non potrà mai rinunciare al lock.
				
			\subsubsection{Algoritmo di Peterson}
				L'idea è di garantire che due thread non entrino mai in sezione critica allo stesso tempo.
				\begin{lstlisting}[style=CStyle]
int flag[2];
int turn;

void init() {
	flag[0] = flag[1] = 0;	// 1->thread wants to grab lock
	turn = 0;								// whose turn? (thread 0 or 1?)
}

void lock() {
	flag[self] = 1;		// self: thread ID of caller
	turn = 1 - self;	// make it other threads turn
	while ((flag[1-self] == 1) && (turn == 1 - self))
		; 							// spin-wait
}

void unlock() {
	flag[self] = 0;		// simply undo your intent
}				\end{lstlisting}
				Se una cella di \texttt{flag} viene settata a 1 indica che il corrispondente thread desidera entrare in sezione critica. \texttt{turn} indica il turno del thread per entrare in sezione critica.
				
			\subsubsection{Spin locks}
				\paragraph{Vantaggi} 
				\begin{itemize}
					\item Semplicità (poche righe di codice).
					\item \textbf{Correctness.} Fornisce la mutua esclusione correttamente.
				\end{itemize}								
				
				\paragraph{Svantaggi} 
					\begin{itemize}
						\item \textbf{Fairness} Non siamo in grado di garantire che ogni thread entrerà in sezione critica.
						\item \textbf{Performance} In una CPU monoprocessore lo spin lock è molto costoso. Quando abbiamo $N$ threads a contendersi il lock, nel caso peggiore verranno sprecato $N-1$ fette di tempo. In altri casi funziona ragionevolmente (\textit{se il numero di threads è più o meno uguale al numero dei processori}).
					\end{itemize}
					
			\subsubsection{Soluzioni}
				\paragraph{Dare la precedenza} ad altri thread invece che fare spinlock.
				\begin{lstlisting}[style=CStyle]
void init() {
	flag = 0;
}

void lock() {
	while (TestAndSet(&flag, 1) == 1)
		yield(); 	//rilascia la cpu
}

void unlock() {
	flag = 0;
}				\end{lstlisting}
				La primitiva \texttt{yield()} è una semplice \textbf{system call} che muove il chiamante dallo stato di running a quello di ready (\textit{deschedula il thread}). Con due thread funziona bene ma con tanti che si contendono la sezione critica no. Se un thread acquisisce la CPU e viene interrotto prima di chiamare la \texttt{unlock()}, tutti gli altri chiameranno la \texttt{lock()}, troveranno il lock occupato e chiameranno la \texttt{yield()}. Oltre al problema della \textbf{starvation }abbiamo anche problemi di \textbf{performance}.
				
				\paragraph{Sleeping instead of spinning} Lo scheduler viene lasciato troppo al caso. \texttt{Solaris} mette a disposizione due routines:
				\begin{itemize}
					\item \texttt{park()} per mettere un thread chiamante in stato di sleep.
					\item \texttt{unpark(threadID)} per svegliare un particolare thread.
				\end{itemize}
				Queste due routines possono essere usate per costruire un lock che mette il chiamante a dormire se il lock è già acquisito e lo sveglia quando è disponibile. Per evitare la starvation si usa una \textbf{coda} per controllare chi è il prossimo a prendere il lock. Inoltre, viene usata una variabile \texttt{guard} per fare spin-lock attorno a \texttt{flag} e manipolare la coda in uso dal lock. Un thread potrebbe comunque essere interrotto durante l'acquisizione o il rilascio del lock, causando gli altri thread a fare spin-wait ancora una volta. Tuttavia, il tempo speso a fare spinning è limitato (solo poche istruzioni all'interno del codice di \texttt{lock} e \texttt{unlock}.
				
				\begin{lstlisting}[style=CStyle]
typedef struct __lock_t {
	int flag;
	int guard;
	queue_t *q;
} lock_t;

void lock_init(lock_t *m) {
	m->flag = 0;
	m->guard = 0;
	queue_init(m->q);
}

void lock(lock_t *m) {
	while (TestAndSet(&m->guard, 1) == 1)
		; //acquire guard lock by spinning
	if (m->flag == 0) {
		m->flag = 1; // lock is acquired
		m->guard = 0;
	} else {
		queue_add(m->q, gettid());
		m->guard = 0;
		park();
	}
}

void unlock(lock_t *m) {
	while (TestAndSet(&m->guard, 1) == 1)
		; //acquire guard lock by spinning
	if (queue_empty(m->q))
		m->flag = 0; // let go of lock; no one wants it
	else
		unpark(queue_remove(m->q)); // hold lock (for next thread!)
	m->guard = 0;
}				\end{lstlisting}				
				
				Se \texttt{guard} fosse dopo \texttt{park()} tutti i thread successivi avrebbero trovato \texttt{guard} a 1, andando in spin-lock.
				conseguentemente, lo scheduler sceglie i threads in modo da garantire che non avvenga nessun
deadlock.
				\texttt{flag} non viene settata nuovamente a 0 quando un altro thread viene svegliato perchè passiamo il lock direttamente dal thread che lo rilascia al prossimo che lo acquisisce. 
				
				È Possibile che un thread sul punto di fare \texttt{park()}	venga switchato al thread in possesso del lock e che quest'ultimo lo rilasci. Ciò potrebbe portare allo sleep permanente del primo thread (\textbf{waiting race}). Si può risolvere con una terza chiamata a \texttt{setpark()} (introdotto da \texttt{Solaris}) che serve per indicare che un thread è in procito di fare \texttt{park()}. Se quindi dovesse avvenire un thread switch e un altro thread chiamasse \texttt{unpark()} prima che \texttt{park()} sia effettivamente chiamata, la successiva \texttt{park()} ritorna immediatamente invece di dormire.
				
				\begin{lstlisting}[style=CStyle]
queue_add(m->q, gettid());
setpark();
m->guard = 0;	\end{lstlisting} 
		
		\subsection{Condition Variables}
			Ci sono molti casi in cui un thread desidera controllare quando una condizione è vera prima di continuare la propria esecuzione. Per esempio un thread genitore potrebbe voler attendere il completamento del figlio prima di continuare (\texttt{join()}).
			
			\begin{lstlisting}[style=CStyle]
volatile int done = 0;

void *child(void *arg) {
	printf("child\n");
	done = 1;
	return NULL;
}

int main(int argc, char *argv[]) {
	printf("parent: begin\n");
	pthread_t c;
	Pthread_create(&c, NULL, child, NULL); // create child
	while (done == 0)
		; // spin
	printf("parent: end\n");
	return 0;
}			\end{lstlisting}
			Usare variabili condivise funziona ma è molto inefficiente, siccome il genitore spreca tempo di CPU a fare spin. Una \textbf{condition variable} è una coda esplicita in cui i threads possono mettersi quando la condizione di esecuzione non è quella desiderata. Quando lo stato cambia, il thread (uno o più) viene svegliato e può quindi riprendere la propria esecuzione.
			Una condition variable ha associate due operazioni: 
			\begin{itemize}
				\item \texttt{wait()} Mette in sleep un thread.
				\item \texttt{signal()} Viene usata quando un thread ha cambiato qualcosa nel programma e vuole quindi svegliarne uno in sleep che aspettava il verificarsi di quella condizione. 
			\end{itemize}
			La \texttt{pthread\_cond\_wait( pthread\_cond\_t *c, pthread\_mutex\_t *m)} prende anche un mutex come parametro. Si assume che questo mutex sia \textbf{locked} quando la \texttt{wait()} viene invocata. La responsabilità della wait è di liberare il lock e mettere il thread chiamante in stato di sleep (atomicamente). Quando un thread viene svegliato, deve acquisire nuovamente il lock prima di ritornare dalla \texttt{wait()} (per prevenire la race condition). Una soluzione al problema del \texttt{join()}:
			\begin{lstlisting}[style=CStyle]
int done = 0;
pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t c = PTHREAD_COND_INITIALIZER;

void thr_exit() {
	Pthread_mutex_lock(&m);
	done = 1;
	Pthread_cond_signal(&c);
	Pthread_mutex_unlock(&m);
}

void *child(void *arg) {
	printf("child\n");
	thr_exit();
	return NULL;
}

void thr_join() {
	Pthread_mutex_lock(&m);
	while (done == 0)
		Pthread_cond_wait(&c, &m);
	Pthread_mutex_unlock(&m);
}

int main(int argc, char *argv[]) {
	printf("parent: begin\n");
	pthread_t p;
	Pthread_create(&p, NULL, child, NULL);
	thr_join();
	printf("parent: end\n");
	return 0;
}			\end{lstlisting}
			\subsubsection{Approcci sbagliati}
				\paragraph{Senza la variabile done} se il figlio viene eseguito immediatamente e chiama la \texttt{thr\_exit()}, essa chiamerà la \texttt{signal()}, ma non ci sono thread in sleep sulla condizione. Quando il genitore verrà eseguito chiamerà la \texttt{wait()} e rimarrà bloccato, nessun thread lo sveglierà mai. 
			
				\paragraph{Senza il lock} se il genitore chiama \texttt{thr\_join()} e controlla il valore di done, vedrà che è a zero e si metterà in sleep. Prima di chiamare la wait viene interrotto e viene eseguito il figlio. Quest'ultimo cambia \texttt{done} a 1 e chiama \texttt{signal()} ma non c'è nessun thread in attesa del verificarsi della condizione. Quando il genitore viene nuovamente eseguito, andrà a dormire per sempre.
			
				Vanno usati sempre i \textbf{cicli while} per i controlli.
			
			\subsubsection{Produttore e consumatore}
				I thread produttori generano i dati e li inseriscono in un buffer, i consumatori prendono questi dati dal buffer e li consumano in un certo modo. Il buffer è una risorsa condivisa ed è necessario sincronizzare l'acesso ad essa per evitare la race condition.
				\begin{lstlisting}[style=CStyle]
int buffer;
int count = 0; // initially, empty

void put(int value) {
	assert(count == 0);
	count = 1;
	buffer = value;
}

int get() {
	assert(count == 1);
	count = 0;
	return buffer;
}				\end{lstlisting}
				\begin{itemize}
				\item \texttt{put()}, assumendo che il buffer sia vuoto, inserisce semplicemente un valore in esso e lo marca come "pieno" settando la variabile \texttt{count}. 
				\item \texttt{get()} fa l'opposto, settando il buffer a vuoto (\texttt{count = 0}) e restituisce il valore prelevato.
				\end{itemize}
				\begin{lstlisting}[style=CStyle]
cond_t cond;
mutex_t mutex;

void *producer(void *arg) {
	int i;
	for (i = 0; i < loops; i++) {
		Pthread_mutex_lock(&mutex);
		if (count == 1)			//questo verra' cambiato con un while
			Pthread_cond_wait(&cond, &mutex);
		put(i);
		Pthread_cond_signal(&cond);
		Pthread_mutex_unlock(&mutex);
	}
}

void *consumer(void *arg) {
	int i;
	for (i = 0; i < loops; i++) {
		Pthread_mutex_lock(&mutex);
		if (count == 0)			//questo verra' cambiato con un while
			Pthread_cond_wait(&cond, &mutex);
		int tmp = get();
		Pthread_cond_signal(&cond);
		Pthread_mutex_unlock(&mutex);
		printf("%d\n", tmp);
	}
}				\end{lstlisting}
				Con un singolo produttore e un singolo consumatore, il codice sopra funziona, ma se abbiamo ad esempio due consumatori, la soluzione due problemi critici:
				\begin{itemize}
					\item Supponiamo che ci siano due consumatori e un produttore:
					\begin{itemize}
						\item $C_1$ acquisisce il lock e controlla che ogni buffer sia pronto per la consumazione ma non ce ne sono, chiama la \texttt{wait}. 
						\item $P_1$ viene eseguito, acquisisce il lock e controlla che tutti buffer siano pieni, ma non lo sono. Va avanti riempendo il buffer. Invoca la \texttt{signal()} per informare che il buffer è stato riempito.
						\item $C_1$ si muove nella coda di ready dallo stato di sleeping sulla condition variable (\textit{non viene ancora eseguito}). 
						\item $P_1$ continua fino a quando non realizza che il buffer è pieno, e poi va in sleep.
						\item $C_2$ viene eseguito e consuma il valore nel buffer (salta la \texttt{wait} perchè il buffer è pieno).
						\item $C_1$ viene ora eseguito, prima di ritornare dalla \texttt{wait()}, acquisisce nuovamente il lock, chiama la \texttt{get()} ma \textbf{non ci sono buffer da consumare}.
						
						Il problema è questo: dopo che il produttore sveglia $C_1$, ma prima che esso venga eseguito, lo stato del buffer è cambiato (per colpa di $C_2$). Segnalare un thread lo sveglia solamente dicendogli che lo stato è cambiato (in questo caso che un valore è stato messo nel buffer), ma non ci sono garanzie che quando il thread svegliato venga eseguito lo stato sia lo stesso (\textbf{Mesa semantics}). 
						
						Soluzione: cambiare l'\textbf{if} in \textbf{while} in modo che se $C_1$ viene svegliato ricontrolla immediatamente lo stato della variabile condivisa. Se il buffer è vuoto, tornerà semplicemente a dormire.
					\end{itemize}
					\item C'è una sola condition variable.
					\begin{itemize}
						\item Vengono eseguiti prima $C_1$ e $C_2$ e vanno in sleep.
						\item Il produttore mette un valore nel buffer e sveglia uno dei consumatori, diciamo $C_1$.
						\item Il produttore torna indietro e prova a inserire più dati nel buffer, siccome è pieno il produttore chiamerà \texttt{wait()} e andrà a dormire.
						\item $C_1$ è pronto per essere eseguito e \textbf{due threads stanno dormendo sulla condizione} ($C_2$ e $P_1$).
						\item $C_1$ Si sveglia ritornando dalla \texttt{wait()}, controlla nuovamente la condizione e trova che il buffer è pieno. Consuma il valore e segnala la condizione, svegliando un thread in sleeping. \textbf{Quale dei due sveglia?} Se svegliasse il consumatore, troverà il buffer vuoto e si metterà in sleep. Il produttore viene lasciato a dormire. Tutti e tre i threads sono in stato di sleeping. Un \textbf{consumatore non dovrebbe poter svegliare altri consumatori}, ma solo produttori (e viceversa).
					\end{itemize}
					La soluzione è usare due condition variables per segnalare correttamente quale tipo di thread andrebbe svegliato.
				\end{itemize}
				\begin{lstlisting}[style=CStyle]
int buffer[MAX];
int fill = 0;
int use = 0;
int count = 0;

void put(int value) {
	buffer[fill] = value;
	fill = (fill + 1) % MAX;
	count++;
}

int get() {
	int tmp = buffer[use];
	use = (use + 1) % MAX;
	count--;
	return tmp;
}

cond_t empty, fill;
mutex_t mutex;

void *producer(void *arg) {
	int i;
	for (i = 0; i < loops; i++) {
		Pthread_mutex_lock(&mutex);	
		while (count == MAX)
			Pthread_cond_wait(&empty, &mutex); 
		put(i);
		Pthread_cond_signal(&fill);
		Pthread_mutex_unlock(&mutex);
	}
}

void *consumer(void *arg) {
	int i;
	for (i = 0; i < loops; i++) {
		Pthread_mutex_lock(&mutex);
		while (count == 0)
			Pthread_cond_wait(&fill, &mutex);
		int tmp = get();
		Pthread_cond_signal(&empty);
		Pthread_mutex_unlock(&mutex);
		printf("%d\n", tmp);
	}
}				\end{lstlisting}
				I thread produttori aspettano sulla condizione \texttt{empty} e segnalano \texttt{fill}, per i consumatori l'inverso. Inoltre per abilitare più concorrenza ed efficienza si aggiungono più slot al buffer, in modo tale che più valori possano essere prodotti o consumati prima di andare in sleep. Un produttore dorme solo se tutti i buffer sono pieni, un consumatore dorme solo se tutti i buffer sono vuoti.
		\subsection{Semafori}
			Un semaforo è un oggetto con un valore di tipo integer utilizzabili sia come locks che come condition variables.
			\\Il valore iniziale di un semaforo ne determina il comportamento.
			\begin{lstlisting}[style=CStyle]
#include <semaphore.h>
sem_t s;
sem_init(&s, 0, 1);			\end{lstlisting}
			Il secondo valore di \texttt{sem\_init()} viene settato a 0 e indica che il semaforo è condiviso tra thread dello stesso processo. Il terzo argomento è il valore con cui lo si inizializza. Per interagire con esso vengono introdotte due routines: 
			\begin{itemize}
				\item \texttt{sem\_wait(sem\_t *s)} decrementa il valore del semaforo \texttt{s} di uno e aspetta se il valore del semaforo è negativo.
				\item \texttt{sem\_post(sem\_t *s)} incrementa il valore del semaforo \texttt{s} di uno, se ci sono uno o più threads in attesa ne sveglia uno.
			\end{itemize}
			
			Il valore del semaforo, quando negativo, è uguale al numero di thread in attesa.
			\subsubsection{Semafori binari: locks}
				Per implementare i lock, il codice sarà: 
				\begin{lstlisting}[style=CStyle]
sem_t m;
sem_init(&m, 0, 1); //inizializza il semaforo a 1
sem_wait(&m);
//sezione critica
sem_post(&m); 	\end{lstlisting}
				Il valore di partenza del semaforo \texttt{m} a 1  è critico per la realizzazione del lock. I semafori \textbf{binari} possono trovarsi solamente in due  stati: acquisito o disponibile. 
			
			\subsubsection{Semafori per ordinare}
				I semafori possono essere usati anche come condition variables, esempio: 
				\begin{lstlisting}[style=CStyle]
sem_t s;

void *child(void *arg) {
	printf("child\n");
	sem_post(&s); // signal here: child is done
	return NULL;
}

int main(int argc, char *argv[]) {
	sem_init(&s, 0, 0); //Inizializzo a 0 per forza
	printf("parent: begin\n");
	pthread_t c;
	Pthread_create(c, NULL, child, NULL);
	sem_wait(&s); // wait here for child
	printf("parent: end\n");
	return 0;
}				\end{lstlisting}
				In questo modo ci aspettiamo di avere come risultato: \texttt{parent: begin, child, parent: end}
			
			\subsubsection{Semafori come produttore e consumatore}
				Si usano due semafori, \texttt{empty} e \texttt{full}, che indicano quando una entry del buffer è stata svuotata o riempita rispettivamente.
				\begin{lstlisting}[style=CStyle]
int buffer[MAX];
int fill = 0;
int use = 0;

void put(int value) {
	buffer[fill] = value;
	fill = (fill + 1) % MAX; 
}

int get() {
	int tmp = buffer[use];
	use = (use + 1) % MAX;
	return tmp;
}				\end{lstlisting}
				Produttori e consumatori: 
				\begin{lstlisting}[style=CStyle]
sem_t empty;
sem_t full;

void *producer(void *arg) {
	int i;
	for (i = 0; i < loops; i++) {
		sem_wait(&empty);
		put(i);
		sem_post(&full);
	}
}

void *consumer(void *arg) {
	int i, tmp = 0;
	while (tmp != -1) {
		sem_wait(&full);
		tmp = get();
		sem_post(&empty);
		printf("%d\n", tmp);
	}
}

int main(int argc, char *argv[]) {
// ...
sem_init(&empty, 0, MAX); //MAX buffers are empty to begin with
sem_init(&full, 0, 0); // ... and 0 are full
// ...
}				\end{lstlisting}
				Il produttore aspetta che il buffer diventi vuoto per poterlo riempire di dati e il consumatore attende che il buffer sia pieno prima di consumare i dati. Con un singolo consumatore e un produttore funziona bene, con \texttt{MAX = 10} abbiamo un problema di \textbf{race condition}:
				\begin{itemize}
					\item Abbiamo due produttori in procinto di chiamare la \texttt{put()}
					\item $P_1$ viene eseguito prima e inizia a riempire il buffer. Prima che esso incrementi \texttt{fill} a 1, viene interrotto. 
					\item Il produttore $P_2$ inizia ad essere eseguito e anche lui, inserisce i suoi dati nello stesso punto del buffer sovrascrivendo quelli vecchi. 
				\end{itemize}
				Manca la \textbf{mutua esclusione}, riempire il buffer e incrementare il contatore è una porzione di codice critica. Basta aggiungere i semafori binari come locks. Inizialmente può sembrare una buona idea metterli intorno a \texttt{wait} e \texttt{post}, ma questo potrebbe portare a un \textbf{deadlock}. 
				\begin{itemize}
					\item Il consumatore viene eseguito e acquisisce mutex. 
					\item Chiama la \texttt{sem\_wait(\&full)}. Possiede ancora il lock.
					\item Viene eseguito un produttore che, se fosse possibile, riempirebbe il buffer di dati e sveglierebbe il consumatore. Chiama \texttt{sem\_wait(\&mutex)}. Il lock è già in possesso del consumatore e il produttore è bloccato ad aspettare. Il consumatore ha il lock ed è in waiting, produttore non ha il lock, ed è in waiting.
				\end{itemize}
				Si sposta quindi il mutex intorno solo alla \texttt{get()} e alla \texttt{put()} e si ottiene il seguente codice: 
				\begin{lstlisting}[style=CStyle]
sem_t empty;
sem_t full;
sem_t mutex;

void *producer(void *arg) {
	int i;
	for (i = 0; i < loops; i++) {
		sem_wait(&empty);
		sem_wait(&mutex);	//(MOVED MUTEX HERE...)
		put(i);
		sem_post(&mutex);	//(... AND HERE)
		sem_post(&full);
	}
}

void *consumer(void *arg) {
	int i;
	for (i = 0; i < loops; i++) {
		sem_wait(&full);
		sem_wait(&mutex);	//(MOVED MUTEX HERE...)
		int tmp = get();
		sem_post(&mutex);	//(... AND HERE)
		sem_post(&empty);
		printf("%d\n", tmp);
	}
}

int main(int argc, char *argv[]){
	// ...
	sem_init(&empty, 0, MAX);//MAX buffers are empty to begin with
	sem_init(&full, 0, 0);   // ... and 0 are full
	sem_init(&mutex, 0, 1);  // mutex=1 because it is a lock
	// ...
}				\end{lstlisting}
			\subsubsection{Implementazione dei semafori}
				\begin{lstlisting}[style=CStyle]
typedef struct __Zem_t {
	int value;
	pthread_cond_t cond;
	pthread_mutex_t lock;
} Zem_t;

// only one thread can call this
void Zem_init(Zem_t *s, int value) {
	s->value = value;
	Cond_init(&s->cond);
	Mutex_init(&s->lock);
}

void Zem_wait(Zem_t *s) {
	Mutex_lock(&s->lock);
	while (s->value <= 0)
		Cond_wait(&s->cond, &s->lock);
	s->value--;
	Mutex_unlock(&s->lock);
}

void Zem_post(Zem_t *s) {
	Mutex_lock(&s->lock);
	s->value++;
	Cond_signal(&s->cond);
	Mutex_unlock(&s->lock);
}				\end{lstlisting}

		\subsection{Problemi relativi alla concorrenza}
			\subsubsection{Non-deadlock bugs}
				\paragraph{Atomicity-violation bug} consiste nel non mettere dei \textbf{locks} attorno ad una variabile condivisa.
				\paragraph{Order-violation bug} consiste nel non assegnare un ordine a due thread che accedono alla stessa variabile. Ad esempio, un thread potrebbe dare per scontato che una variabile sia già inizializzata ed utilizzarla, ma se è ancora a \texttt{NULL} questo causerà un crash. Per risolvere questo bug, basta usare le \textbf{condition variables}.
				
			\subsubsection{Deadlock bugs}
				I deadlock avvengono quando un thread $T_1$ è in possesso del lock $L_1$ e in attesa di un altro $L_2$; il thread $T_2$ che possiede il lock $L_2$ è in attesa che $L_1$ venga rilasciato.
				\begin{lstlisting}[style=CStyle]
Thread 1:									Thread 2:
pthread_mutex_lock(L1);		pthread_mutex_lock(L2);
pthread_mutex_lock(L2);		pthread_mutex_lock(L1);\end{lstlisting}
				Le ragioni per cui i deadlock avvengono possono essere: 
				\begin{itemize}
					\item Troppo codice
					\item Dipendenze complesse tra i componenti.
					\item L'incapsulamento del codice.
				\end{itemize}
				Affinchè avvenga un deadlock devono essere valide quattro condizioni: 
				\begin{itemize}
					\item \textbf{Mutua esclusione} I threads richiedono il controllo escluso delle risorse che acquisiscono.
					\item \textbf{Hold-and-wait} I thread tengono le risorse allocate da essi (ad esempio i locks che hanno già acquisito) finchè aspettano risorse addizionali (ad esempio i lock che vogliono acquisire).
					\item \textbf{No preemption} Le risorse (ad esempio i locks) non possono essere rimosse forzatamente dai threads che le stanno tenendo.
					\item \textbf{Circular wait} Esistono catene circolari di threads.
				\end{itemize}
				\textbf{Se una qualunque di queste condizioni non è soddisfatta, un deadlock non può avvenire.}
			\subsubsection{Prevenzione dei deadlocks}
				\begin{itemize}
					\item \textbf{Circular wait} La miglior tecnica di prevenzione è fornire un acquisizione del lock ordinata. Per esempio, possiamo prevenire il deadlock acquisendo sempre $L_1$ prima di $L_2$. Questo ordine consente di evitare wait cicliche e quindi deadlock.
					\item \textbf{Hold-and-wait} Per evitare il deadlock dovuto a questa condizione basta acquisire tutti i locks in una volta atomicamente, in modo da evitare che non ci siano thread switch prematuri nel mezzo dell'acquisizione del lock. La soluzione però è problematica: richiede di sapere esattamente quali locks devono essere posseduti e di acquisirli tutti in una volta. Tutti i locks devono essere acquisiti in una volta anche se non è necessario possederli tutti.
					\item \textbf{No preemption} Per non dare tutti i locks come acquisiti fino a quando la \texttt{unlock()} non viene invocata, si può usare una routine come \texttt{pthread\_mutex\_trylock()} che prende il lock (se disponibile) e ritorna \textbf{success} o un codice di errore se il lock è già posseduto. Questo per evitare l'acquisizione multipla di locks, perchè potremmo aspettarne uno mentre siamo in possesso di un altro. Sorge un nuovo problema, la \textbf{livelock}. 
					\begin{lstlisting}[style=CStyle]
top:
pthread_mutex_lock(L1);
if(pthread_mutex_trylock(L2) != 0) {
	pthread_mutex_unlock(L1);
	goto top;
}					\end{lstlisting}					
					È possibile che due threads tentino ripetutamente questa sequenza e falliscano nell'acquisire il lock. Si può risolvere con un delay random prima di fare il loop back.
					\item \textbf{Mutua esclusione} Si possono utilizzare itruzioni hardware per evitare di usare i locks, in questo modo non sarebbe possibile che si verifichino deadlocks (livelock può comunque accadere).
				\end{itemize}
				
			\subsubsection{Algoritmo del banchiere}
				Si potrebbe non eseguire concorrentemente threads che potrebbero causare deadlock. Un approccio che utilizza questa tattica è l'algoritmo banchiere, che permette di gestire istanze multiple di una risorsa. 
				
				Sua $n$ il numero di processi e $m$ il numero di tipi di risorse, abbiamo che:
				\begin{itemize}
					\item \texttt{disponibili[j] = k} se ci sono $k$ istanze disponibile del tipo di risorsa $R_j$. Vettore lungo $m$.
					\item \texttt{massimo[i,j] = k} se il processo $P_i$ può richiedere al più $k$ istanze del tipo di risorsa $R_j$. Matrice $n \times m$.
					\item \texttt{assegnate[i,j] = k} se a $P_i$ sono attualmente allocate $k$ istanze di $R_j$. Matrice $n \times m$.
					\item \texttt{necessità[i,j] = k} se $P_i$ può richiedere $k$ ulteriori istanze di $R_j$ per completare il proprio task. 
				\end{itemize}
				$$\texttt{necessità[i,j] = massimo[i,j] - assegnate[i,j]}$$
				
				\begin{lstlisting}[style=CStyle]
lavoro[m]
fine[n]
lavoro = disponibili

for i = 0 to n:
	fine[i] = false

back:
if si trova i tale che fine[i] = false & richiesta[i] <= lavoro:
	lavoro = lavoro + assegnate[i]
	fine[i] = true
	goto back
	
for i = 0 to n:
	if fine[i] = false:
		il processo i e' in deadlock

il sistema e' in stato sicuro.				\end{lstlisting}
				Se i deadlock sono rari, viene fatto un reboot e basta. Periodicamente un deadlock detector viene eseguito e costruisce un grafico delle risorse e lo controlla per individuare i cicli. Se accade un deadlock, il sistema ha bisogno di essere fatto ripartire.


				

				
			
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage	
	\section{Persistence}
		
		\subsection{I/O devices}
			Più sono corti i bus, più veloci sono. La memoria è collegata alla CPU attraverso il memory bus, altri dispositivi I/O più generici attraverso I/O bus generici (\textit{PCI}). Le periferiche, attraverso peripheral I/O bus (\textit{SATA, USB, \dots}). La bontà di bus si misura in \textbf{banda} (quantità di dati che riesce a far circolare).
			
			\subsubsection{Dispositivo generico}
				Un dispositivo ha due componenti e tre registri: 
				\begin{itemize}
					\item \textbf{Interfaccia} Componente software o hardware con cui comunica con l'esterno.
					\item \textbf{Struttura interna} Parte del dispositivo responsabile dell'implementazione dell'astrazione che il dispositivo presenta al sistema (\textbf{drive}). Possono essere semplici chip o semplici CPU. 
					\item \textbf{Status register} Può essere letto per vedere lo stato corrente del dispositivo.
					\item \textbf{Command register} Usato per ordinare al dispositivo di eseguire un certo compito.
					\item \textbf{Data register} Usato per trasferire dati da e verso il dispositivo. 
				\end{itemize}
				Il \textbf{protocollo} che il sistema operativo usa per interagire con un dispositivo è:
				\begin{itemize}
					\item Polling sullo status register del dispositivo per aspettare che sia pronto a ricevere un comando. 
					\item Manda qualche dato nel data register. Potrebbero essere necessarie diverse writes. Se la CPU principale è coinvolta con il trasferimento dati  è \textbf{programmed I/O}.
					\item Scrive un comando nel command register. Fare ciò permette implicitamente al dispositivo di sapere che i dati sono presenti e che deve iniziare ad eseguire il comando richiesto.
					\item Il sistema operativo aspetta che il dispositivo termini, facendo polling sullo status register.
				\end{itemize}
				Questo protocollo è semplice, funziona ma è inefficiente. 
				
			\subsubsection{Riduzione dell'overhead della CPU con interrupts}
				Per evitare l'overhead dovuto al polling, il sistema operativo avanza una richiesta, mette il processo chiamante in sleep e fa un context switch a un altro processo. Quando il dispositivo ha terminato l'operazione richiesta, solleverà un interrupt hardware (\textbf{interrupt I/O}) che costringerà la CPU a saltare nel S.O. a un \textbf{interrupt handler}. L'handler è un pezzo del codice del sistema operativo che terminerà la richiesta e sveglierà il processo in attesa dell'I/O. 
				
				\textbf{Non è sempre }la soluzione migliore perchè in presenza di un dispositivo rapido, si avrà un rallentamento del sistema dovuto ai molti context switch. In quel caso la soluzione migliore potrebbe essere il polling. Se non è conosciuta la velocità del dispositivo, la scelta migliore potrebbe essere un approccio \textbf{ibrido} (\textit{polling per un po', se il dispositivo non ha finito vengono usati gli interrupts}). 
				
			\subsubsection{DMA - Direct Memory Access}
				Un \textbf{DMA} è un dispositivo specifico che orchestra il trasferimento tra dispositivi e memoria principale senza che la CPU debba intervenire. Viene programmato dal S.O. il quale gli dirà dove risiedono i dati in memoria, quanti dati copiare e a quale dispositivo mandarli. Il sistema operativo ha terminato con il trasferimento e può procedere intanto con altri compiti. Quando il DMA ha  finito, il \textbf{DMA controller} genera un interrput per avvertire il S.O. del completamento del trasferimento.
				\paragraph{Vantaggi} I tempi di CPU sono notevolmente ottimizzati.
				\paragraph{Svantaggi}
				\begin{itemize}
					\item Richiede hardware aggiuntivo.
					\item Il DMA controller ruba cicli di CPU.
					\item Per evitare collisioni (\textit{memoria che serve sia al S.O. che al DMA}) si usa una \textbf{cache}.
					\item Bisogna sincronizzare RAM e DMA per evitare di caricare o scrivere dati non aggiornati. Se il DMA viene interrotto dalla CPU a causa di un page fault e la pagina dove stavano per essere caricati i dati viene messa su disco, dobbiamo evitare che il DMA scriva sulla pagina di un altro processo. Si usa un bit speciale che, se settato a 1, impedisce all'algoritmo di page replacement di toccare la pagina (\textbf{pinning}).
				\end{itemize}
				
			\subsubsection{Interazione con i dispositivi}
				Come fa l'hardware a comunicare con i dispositivo? Esistono due metodo primari per farlo:
				\begin{itemize}
					\item \textbf{Istruzioni eplicite I/O} (\textit{es x86: in, out}) per mandare dati a specifici registri del dispositivo.  Il chiamante specifica un registro con i dati e il nome della porta del dispositivo. Sono istruzioni \textbf{privilegate}.
					\item \textbf{Memory mapped I/O} l'hardware rende i registri dei dispositivi disponibili come se fossero locazioni di memoria vere e proprie. Si usano le solite load e store.
				\end{itemize}
				
			\subsubsection{Device driver}
				Per mantenere la maggior parte del sistema operativo \textit{device-neutral}, nascondendo i dettagli delle interazioni coi dispositivi, al livello più basso, un pezzo di software nel sistema operativo deve conoscere i dettagli di come funziona un dispositivo. Questo pezzo di software è chiamato \textbf{device driver}. L'incapsulamento ha anche aspetti negativi, in presenza di un dispositivo con molte capacità speciali, quest'ultimo dovrà comunque presentare un'interfaccia generica al resto del kernel.
				Siccome i device drivers sono necessari per ogni dispositivo inserito nel sistema, il kernel è composto circa al 70\% di drivers.
				
				Il sistema operativo deve adattarsi ai dispositivi, introducendo i drivers appropriati. Se fosse il contrario, i ssitemi operativi sarebbero molto meno complessi e più compatibili.
				
		\subsection{Hard disks}
			Gli hard disk sono la principale forma di archiviazione persistente dei dati nei sistemi di computer e nello sviluppo della tecnologia dei file systems.
			
			\subsubsection{Geometria base}
				Un disco ha diversi piatti, ogni piatto ha due superfici, ognuna delle quali è divisa in anelli concentrici detti tracce, le quali sono a loro volta suddivise in settori. La velocità di rotazione è misurata in rotazioni per minuto (\textbf{RPM}). Si leggono e scrivono dati attraverso una testina.
				Ogni settore è una fetta da \texttt{512B}. È possibile fare operazioni multi-settore, una singola write da \texttt{512B} viene fatta atomicamente. Se si dovesse verificare una perdita di corrente, solo una write viene scritta male.
				
			\subsubsection{Hard disk semplice}
				Il disco deve aspettare che il settore venga fatto ruotare sotto la testtina (\textbf{rotational delay}). Il processo impegato per muovere il braccio del disco alla traccia corretta è chiamato \textbf{seek}. La seek consiste in: accelerazione, spostamento a piena velocità, decelerazione, e \textbf{settling}. La fase finale dell'I/O (scrittura o lettura) è conosciuta come \textbf{transfer}. 
				$$T_{I/O} = T_{seek} + T_{rotation} + T_{transfer}$$
				$$Rate_{I/O} = Size_{transfer} / T_{I/O}$$
				
			\subsubsection{Disk scheduling}
				Nello scheduler dei processi non eravamo a conoscenza della lunghezza di ogni job, con il disk scheduling possiamo fare buone assunzioni di quanto impiegherà un job. Stimando il seek time e la rotational delay di una richiesta, il \textbf{disk scheduler} può sapere quanto quella richiesta impiegherà per essere soddisfatta. Cercherà di eseguire il principio di \textbf{SJF} \textit{Shortest Job First}.
				
			\subsubsection{SSTF - Shortest Seek Time First}
				\textbf{SSTF} ordina la coda delle richieste di I/O per track, scegliendo le richieste sulla traccia più vicina da eseguire prima. Il problema è che al sistema operativo non è disponibile la geometria del drive, il quale vede il dispositivo come un array di blocchi. Può però implementare un algoritmo \textbf{NBF} \textit{nearest block first}. Inoltre, se ci fosse un flusso costante di richieste alla traccia dove la testina è correntemente posizionata, causerebbe \textbf{starvation}.
				
			\subsubsection{Elevator - SCAN o C-SCAN}
				\textbf{SCAN} si muove avanti e indietro attraverso il disco, servendo le richieste in ordine di tracks. Un passo da una traccia all'altra viene detto \textbf{sweep}. Se si incontra una richiesta per una track che è già stata servita in questo sweep del disco, non viene gestita subito ma viene accodata fino al prossimo sweep. Non è ottimale in quanto non si avvicina al principio di \textbf{SJF} a cui si voleva puntare.
				
			\subsubsection{SPTF - Shortest Positioning Time First}
				Se il tempo di seek è molto più alto di quello di rotational delay, allora SSTF funziona correttamente. Tuttavia, se il tempo di seek fosse più veloce di quello di rotation, allora avrebbe più senso usare un algoritmo che valuti quale settore è meglio servire prima. \textbf{SPTF} (o SATF \textit{shortest access time first}) è utile per migliorare le performance. È molto difficile implementarlo \textbf{in un sistema operativo}, in quanto non ha generalmente un'idea di dove siano collocate le tracks o di dove si trovi la testina del disco. SPTF viene quindi eseguito all'\textbf{interno di un drive}.
				
			\subsubsection{Problemi relativi ai dischi}
				\begin{itemize}
					\item \textbf{Dove fare il disk scheduling?} Nei vecchi sistemi veniva fatto dal sistema operativo, in quelli moderni i dischi possono avere scheduler interni molto sofisticati in grado di implementare SPTF. All'interno del \textbf{disk controller} sono contenuti una serie di dettagli rilevanti, inclusa l'esatta posizione della testina.
					\item \textbf{I/O merging} Se due richieste di lettura sono in due settori adiacenti, l'operazione di I/O merging permette di fonderle in un'unica richiesta. È importante a livello di sistema operativo in quanto riduce il numero di richieste inviate al disco.
					\item \textbf{Quanto tempo si dovrebbe aspettare prima di eseguire un I/O su disco?} Si può servire il disco non appena si presenta una richiesta in modo che il dispositivo non sia mai in stato di IDLE (\textbf{work-conserving}) o si può attendere per una richiesta migliore e più efficiente (\textbf{non-work-conserving}) e in questo modo le performance aumentano notevolmente. 
				\end{itemize}
		
		\subsection{RAID}
			Il sistema \textbf{RAID} (\textit{redundant array of inexpensive disks o redundant array of indipendent disks}) è una tecnica che, utilizzando dischi multipli in accordo, costruisce un disk system più veloce, grande e affidabile. Esternamente un RAID appare come un normale disco, un gruppo di blocchi che possono essere letti e scritti. Internamente invece è molto complesso e consiste in dischi multipli, memoria (volatile e non) e uno o più processori adoperati per la gestione del sistema. I RAIDs offrono molti \textbf{vantaggi}:
			\begin{itemize}
				\item \textbf{Performance} Usando dischi multipli in parallelo è possibile velocizzare notevolmente i tempi di I/O.
				\item \textbf{Capacità} Essendoci molti dischi siamo in grado di memorizzare dati di grandi dimensioni. 
				\item \textbf{Affidabilità} I RAIDs tollerano la perdita di un disco e continuano a operare normalmente.
			\end{itemize}
			I sistemi che adoperano RAID non hanno bisogno di trattare problemi legati alla compatibilità.
			
			\subsubsection{Interfaccia e struttura interna}
				Quando un file system esegue una richiesta \textbf{I/O logica} al RAID, quest'ultimo calcola internamente quale disco (o dischi) devono essere acceduti ed eseguirà uno o più \textbf{I/O fisici} per svolgere il compito richiesto. Un RAID è generalmente costituito da un insieme di dischi \textbf{SCSI (o SATA)} e da un \textbf{SCSI (o SATA) controller}  che esegue il firmware per dirigere le operazioni, una memoria volatile \textbf{DRAM} usata come buffer per blocchi di dati quando essi vengono letti o scritti e, in alcuni casi, una \textbf{memoria non volatile} usata come buffer per scrivere in modo sicuro.
				
				Le idee base di un sistema RAID sono:
				\begin{itemize}
					\item Distribuire l'informazione su più dischi in modo da parallelizzare una parte delle operazioni di accesso ai dati e guadagnare in prestazioni.
					\item Duplicare su più dischi l'informazione memorizzata per maggiore affidabilità. 
				\end{itemize}
				Quando il S.O. vuole accedere a un blocco logico, presenta il numero del blocco al controller del disco. Quest'ultimo utilizza il numero per risalire al settore che contiene il blocco desiderato, ne legge il contenuto e lo trasferisce in RAM (tramite DMA). Esistono diversi livelli di sistemi RAID:
				
			\subsubsection{RAID level 0: Striping}
				Non sono veri e propri RAID, in quanto non viene impiegata alcuna duplicazione dei dati. Il disco virtuale (ciò che viene visto dal S.O.) viene mappato dalla logica del RAID sui vari settori dei dischi, suddividendo i blocchi logici del disco virtuale in \textbf{strips} (una striscia può consistere in più blocchi). Per mappare l'array virtuale nelle locazioni fisiche del disco, dato un indirizzo logico $A$ di un blocco si ha:
				$$Disk = indirizzo \% n\_dischi$$ 
				$$Offset = A / n\_dischi $$
				La parte con la virgola viene troncata. Un RAID di livello 0 è tanto più efficace quanto più le richieste coinvolgono l'accesso a molti blocchi consecutivi e quanto più è alto il numero di dischi su cui sono suddivisi i blocchi. Se le operazioni su disco richiedono l'accesso a dati contenuti nello stesso disco, si hanno le stesse prestazioni di un disco normale. L'affidabilità è inferiore a quella di un semplice disco, perchè è formato da più dischi e il \textbf{Mean time to failure} (MTTF) si abbassa. Viene usato quando si ha bisogno di alte prestazioni senza particolari problemi di affidabilità (\textit{streaming video o audio}).
				
				\paragraph{Performance}
					\begin{itemize}
						\item \textbf{Capacità} dati $N$ dischi, ognuno composto da $B$ blocchi, fornisce $N * B$ blocchi di capacità utile.
						\item \textbf{Affidabilità} bassa, ogni disk failure porterà a una perdita di dati.
						\item \textbf{Performance} eccellente. Tutti i dischi sono utilizzati, spesso in parallelo, per servire le richieste di I/O.
						\item \textbf{Single-block lantecy} uguale a quella di un singolo disco.
						\item \textbf{Steady-state throghput} dati $N$ dischi e $R$ un workload random. Per un elevato numero di I/Os random, possiamo ancora una volta usare tutti i dischi, ottenenendo $N * R \texttt{ MB/s}$ 
					\end{itemize}
					
			\subsubsection{RAID level 1: Mirroring}
				Usa contemporaneamente striping e mirroring: tutti i dati sono suddivisi in strip (come nel livello 0) e duplucati su due dischi. È la soluzione RAID più costosa a parità di capacità di memorizzazione, ma anche la più affidabile e la più efficiente in lettura. I dischi di mirroring possono essere usati per fare letture in parallelo. 
				
				\paragraph{Performance}
					\begin{itemize}
						\item \textbf{Capacità} $N$ dischi e $B$ blocchi con il mirroring level = 2 ha capacità $(N*B)/2$.
						\item \textbf{Affidabilità} eccellente.
						\item \textbf{Performance} elevate. Le richieste possono essere servite in parallelo come per il RAID 0.
						\item \textbf{Single-block latency} In lettura ha la stessa latenza di un disco singolo, in scrittura la write logica deve attendere il completamento di due writes fisiche. L'operazione di scrittura sarà più lenta rispetto a quella in un singolo disco.
						\item \textbf{Steady-state throughput} Quando scriviamo o leggiamo \textbf{sequenzialmente}, ad esempio nel blocco logico 0, il RAID internamente scriverà sui blocchi 0 e 1. La banda massima ottenuta durante una write o una read sequenziale è di $(N/2)*S$.  Il workload random è il caso migliore in quanto con le read è in grado di fornire una banda pari a $N*R \texttt{ MB/s}$. Per quanto riguarda le writes, $(N/2)*R \texttt{ MB/s}$ ognuna.
					\end{itemize}
					
			\subsubsection{RAID level 4: saving space with parity}
				Usa la tecnica di striping a livello di blocchi e introduce anche un disco di parità per eventuali operazioni di \textbf{recovery}. Il blocco $n$ del disco di recovery, conterrà la parità della strip $n$. Per computare la parità si usa la \texttt{XOR bitwise} (\textit{bit a bit}). L'informazione di parità può essere usata per guarire da un fallimento. Per sapere quale contenuto aveva un blocco, semplicemente si leggono tutti i valori nelle righe (incluso il bit di parità).
				
				Il RAID 4 garantisce lo stesso livello di mantenimento dei dati del RAID 1 in caso di guasto di un disco, ma al costo una maggiore inefficienza, in quanto ogni qualvolta che una strip di un disco viene modificata, occorre leggere e ricalcolarne la parità. Inoltre, il disco di recovery è coinvolto in ogni operazione di scrittura sul RAID e può facilmente diventare un \textbf{collo di bottiglia}.
				
				\paragraph{Performance}
					\begin{itemize}
						\item \textbf{Capacità} $(N-1)*B$.
						\item \textbf{Affidabilità} tollera il fallimento su un solo disco, non di più.
						\item \textbf{Performance} basse.
						\item \textbf{Single-block latency} La latenza di una signola write richiede 2 read e 2 write (2 per il dato e 2 per la parità). Le reads possono essere fatte in parallelo, così come le writes. La latenza totale è quindi doppia rispetto al sinolo disco (con qualche differenza visto che dobbiamo aspettare che entrambe le reads vengano portate a termine).
						\item \textbf{Steady-state throughput} Nel caso di scrittura sequenziale in tutta una striscia, la banda equivale a $(N-1)*S \texttt{ MB/s}$. Anche nel caso di una random read, in quanto non abbiamo bisogno di leggere il disco di parità. Per quanto riguarda le random writes invece, dobbiamo aggiornare il valore di parità. Ci sono due metodi: 
							\begin{itemize}
								\item \textbf{Additive parity} per sapere il valore di parità si legge in parallelo il valore dei blocchi nella striscia e si esegue una \texttt{XOR} con il valore del nuovo blocco. Maggiore sono i dischi e più costosa diventa.
								\item \textbf{Subtractive parity} Vengono letti il bit che bisogna cambiare e il bit di parità. Se il bit vecchio è uguale al nuovo, si lascia uguale il bit di parità, altrimenti diventa il suo opposto.
							\end{itemize}
					\end{itemize}
					Quando vogliamo scrivere due dati in due strips diverse e in due dischi diversi, possiamo eseguire la scrittura in parallelo, ma il disco di parità dovrà essere acceduto sequenzialmente. Questo problema è noto come \textbf{small-write problem}. Le performance (\textit{pessime}) di piccole random writes è pari  $(R/2) \texttt{ MB/s}$. 
				
			\subsubsection{RAID level 5: rotation parity}
				Funziona come il 4, ma per ridurre il carico sul disco di parità nelle operazioni di scrittura, distribuisce i blocchi di parità fra i vari dischi. Il difetto di questo approccio è che, in caso di guasto di un disco, è più complessa la ricostruzione. Questo livello fornisce comunque la migliore combinazione in termini di prestazioni, affidabilità e capacità di memorizzazione. La parità viene fatta ruotare in tutti i dischi in modo da eliminare l'effetto \textbf{collo di bottiglia} del livello 4.
				
				\paragraph{Performance}
					\begin{itemize}
						\item \textbf{Capacità} $(N-1)*B$.
						\item \textbf{Affidabilità} Uguale al RAID 4.
						\item \textbf{Performance} Rispetto al RAID 4 ha performance notevolmente migliorate grazie alla disposizione dei blocchi di parità all'interno dei dischi.
						\item \textbf{Single-write latency} Uguale al RAID 4.
						\item \textbf{Steady-state throughput} Nel workload random, il RAID 5 funziona leggermente meglio rispetto al RAID 4 perchè ora possiamo utilizzare tutti i dischi. Le performance di una write random è notevolmente migliorata visto che ora siamo in grado di parallelizzare le richieste e non dobbiamo più accedere sequenzialmente al disco di parità. Dato un elevato numero di richieste random, saremo in grado di tenere occupati praticamente tutti i dischi. Se questo è il caso, allora la larghezza di banda per piccole writes sarà di $(N/4)*R \texttt{ MB/s}$. Il fattore di perdità 4 è dovuto al fatto che ogni write genera 4 operazioni di I/O, che è semplicemente il costo dovuto all'impiego della parità (\textit{read data, read parity, write data, write parity}). 
					\end{itemize}
			
			\subsubsection{Riassunto prestazioni}
				\begin{center}				
				\begin{tabular}{ccccc}
					& \textbf{RAID 0} & \textbf{RAID 1} & \textbf{RAID 4} & \textbf{RAID 5}\\
					\hline
					Capacity & $N*B$ & $(N*B)/2$ & $(N-1)*B$ & $(N-1)*B$\\		
					\hline
					Reliability & 0 & $1_{sure}$ $\frac{N}{2}_{if lucky}$ & 1 & 1 \\
					\hline
					Throughput &&&&\\
					- Sequential read & $N*S$ & $(N/2)*S$ & $(N-1)*S$ & $(N-1)*S$\\
					- Sequential write & $N*S$ & $(N/2)*S$ & $(N-1)*S$ & $(N-1)*S$\\
					- Random read & $N*R$ & $N*R$ & $(N-1)R$ & $N*R$\\
					- Random write & $N*R$ & $(N/2)*R$ & $\frac{1}{2}R$ & $\frac{N}{4}R$\\
					\hline
					Latency \\
					- Read & T & T & T & T \\
					- Write & T & T & 2T & 2T
				\end{tabular}
				\end{center}
		
		\subsection{File e directories}
			Il file è l'elemento alla base della memorizzazione. È un array lineare di bytes che può essere letto o scritto. I file sono contraddistinti da un low-level name noto come \textbf{inode number}.
			
			Una \textbf{directory}, così come un file, ha un inode number associato. Il suo contenuto è una lista di coppie "user-readable-name"/"low-level-name". La directory principale è chiamata \textbf{root} ( / ), le altre sono chiamate sub-directories. Files e directories possono avere lo stesso nome a patto che siano in nodi diversi dell'albero delle directory.
			
			\subsubsection{Creazione di un file}
				È possibile creare file in svariati modi, la pratica più diffusa è l'invocazione della system call \textbf{open} utilizzando la flag \texttt{O\_CREAT} che prende come argomenti il nome del file, e una serie di parametri per specificare i permessi e il tipo di operazioni eseguibile sul file. 
				$$ \texttt{int fd = open("foo",O\_CREAT|O\_WRONLY|O\_TRUNC,S\_IRUSR|S\_IWUSR)} $$
				La open ritorna un intero definito \textbf{file descriptor}. Il file descriptor è un indice utilizzato dal file system per individuare un file aperto nella \textbf{file table}.
				
			\subsubsection{Lettura e scrittura (sequenziale) di un file}
				Ogni processo ha 3 file aperti, \textbf{standard input, output ed error}. Essi sono rappresentati dai file descriptors 0, 1 e 2. Per leggere un file viene invocata la \texttt{open()} e poi la \texttt{read()} per leggere ripetutamente i bytes dal file. Il primo argomento di \texttt{read()} è il file descriptor, il secondo è un puntatore a un buffer dove verrà posizionato il risultato della chiamata, il  terzo argomento è la dimensione del buffer. Una volta terminata la \texttt{read()}, restituisce il numero di bytes letti e viene invocata la \texttt{write()} con argomento il file descriptor 1 (std output). Il secondo argomento è ciò che deve stampare, cioè i bytes appena letti dalla \texttt{read()}, il terzo è la grandezza del buffer da stampare. Infine viene chiamata nuovamente la \texttt{read()} e se quest'ultima restituisce 0, specifica che non ci sono altri bytes da leggere e viene quindi invocata la \texttt{close()}. Il procedimento di scrittura di un file è pressochè identico.
				
			\subsubsection{Lettura e scrittura (non sequenziale) di un file}
				Per poter accedere direttamente alla posizione designata viene adoperato un \textbf{random offset} utilizzato per spostarsi all'interno del file. I possibili valori di whence (\textit{da dove}) possono essere:
				\begin{itemize}
					\item \textbf{SEEK-CURE} ci si sposta di offset posizioni dal valore attuale. Ad esempio se sono al byte 50 e offset è pari a 100, la posizione di arrivo sarà il byte 150.
					\item \textbf{SEEK-END} Ci si sposta di offset posizioni dalla fine del file. Ad esempio \texttt{lseek(fd, 10, SEEK\_END)} la posizione di arrivo sarà ottenuta spostandoci di 10 posizioni dalla fine del file \texttt{fd}.
					\item \textbf{SEEK-SET} viene spostato l'indice all'ofset specificato. Ad esempio, con offset pari a 50, la \texttt{lseek} si sposta al byte 50.
				\end{itemize}
				
			\subsubsection{Scrittura immediata su disco}
				Quando viene invocata la write(), il chiamante sta generalmente chiedendo di salvare in modo persistente certi dati. Il file system, per questioni di performance, bufferizzerà queste writes in memoria per un certo istante di tempo (dai 5 ai 30 secondi). Dal punto di vista del chiamante, le richieste sembrano essere servite rapidamente e solamente in rari casi (ad esempio a seguito di un crash dopo l’invocazione della syscall \texttt{write()} ma subito prima che i dati vengano scritti su disco) vi sarà una perdita d’informazione. Tuttavia, alcune applicazioni richiedono più di questa “eventuale garanzia”. I file systems forniscono, per le applicazioni che necessitano evitare perdita di dati, APIs di controllo addizionali. Nel mondo Unix, l’interfaccia usata per servire a questo scopo è conosciuta come \texttt{fsync(int fd)}. Quando un processo invoca la \texttt{fsync()} per un certo file descriptor, il file system risponde forzando tutti i dati non ancora scritti (\textit{dirty}) su disco. La \texttt{fsync()} ritorna una volta che tutte le writes sono state completate.
				
			\subsubsection{Rinominare un file}
				Per rimominare un file, basta usare la move. \texttt{mv foo bar} invoca la \texttt{open()} per creare un file temporaneo, viene scritta la nuova versione del file attraverso la \texttt{write()} e in fine viene forzata la scrittura su disco con \texttt{fsync()}. Quando si è certi di aver salvato il nuovo nome, viene rinominato il file temporaneo col nome effettivo (questo passaggio viene fatto atomicamente grazie alla "rename").
				
			\subsubsection{Informazioni dei file}
				Il file system mantiene una serie di  informazioni note come \textbf{metadati} relative ad ogni file memorizzato. Per visionare i metadati di un file, si possono utilizzare le system call \texttt{stat()} o \texttt{fstat()}. I file system tengono i metadati in strutture chiamate \textbf{inode}. 
				
			\subsubsection{Rimozione dei file}
				Per rimuovere un file viene usato il comando \texttt{rm}. Questo chiama la system call \texttt{unlink()}, che riceve come argomento il nome del file da rimuovere e restituisce 0 in caso di successo. 
				
			\subsubsection{Creazione directories}
				Non è possibile scrivere direttamente in un directory, questo perchè il suo formato è considerato un metadato del file system. Per creare una directory viene adoperata la system call \texttt{mkdir()}. Quando viene creata è considerata vuota. Nello specifico contiene due entries: ( . , se stessa), ( .. , direcotry genitore).
				
			\subsubsection{Leggere le directories}
				Per leggere il contenuto di una direcotry si utilizza il comando \texttt{ls} che invoca le syscall \texttt{opendir(), readdir()} e \texttt{closedir()}.
				
			\subsubsection{Eliminare directories}
				Per eliminare una directory viene invocata la system call \texttt{rmdir()}. Prende come argomento il nome (user-readable-name) della directory che si vuole eliminare e, se essa è vuota serve la richiesta, altrimenti esce stampando un messaggio di errore. Questo per evitare perdita di dati importanti.
				
			\subsubsection{Hard links}
				La syscall \texttt{link()} riceve due argomenti: un vecchio e un nuovo pathname. Quando linchiamo un nuovo file-name a uno vecchio, essenzialmente stiamo creando un nuovo modo per riferirsi allo stesso file. Viene usato il prgramma \texttt{ln}.
				
				Un \textbf{hard link} è l'associazione del nome di un file ad un inode del sistema. Quando creiamo un file, viene creata una struttura inode che terrà traccia di una serie di informazioni relative a tale file (grandezza, locazione dei suoi blocchi su disco, \dots), e successivamente viene collegato (link) uno human-readable name a quel file. Infine il link viene posizionato nella directory in cui il file è stato creato. Il file system mantiene un contatore noto come \textbf{link count} associato a ciascun inode del sistema. Se è a 0, il file system libera l'inode e i blocchi di dati relativi, eliminando realmente il file.
				
			\subsubsection{Symbolic links}
				Gli hard link sono limitati in quanto non è possibile creare un hard link a una directory (potrebbe generare un ciclio all'interno dell'albero delle directory) e non è possibile creare un hard link a un file in altri partizioni del disco (gli inode number sono unici all'interno di un particolare file system).
				Un symbolic link (\texttt{ln -s}) è un file vero e proprio e sono il terzo tipo di dati che il file system conosce (\textit{file, directory e soft link}). Il contenuto del collegamento è il pathname al file a cui fa riferimento e non i dati effettivi. Se si elimina il filename originale, il collegamento punta a un pathname inesistente (\textbf{dangling reference}).
				
			\subsubsection{Making and mounting the file system}
				Come assembliamo un unico directory tree a partire da molti file systems? Un \textbf{file system} è un meccanismo mediante il quale i file sono collocati e organizzati su un dispositivo di archiviazione. Per creare un file system viene fornito un tool \texttt{mkfs}. A seguito di questo comando siamo in grado di creare un file system vuoto sul device passatogli come argomento. Per rendere l'albero accessibile si usa il comando \texttt{mount} (copia il file system tree e lo incolla nel punto specificato in mount).  
				 
		\subsection{File system implementation}
			Un file system è una componente \textbf{puramente software}.
			
			\subsubsection{VSFS - very simple file system}
				La prima cosa da fare è dividere il disco in $N$ blocchi (\textit{es} \texttt{4KB}) indirizzabili da $0$ a $N-1$. Lo spazio complessivo sarà pari a $N * \texttt{4KB}$. Questi blocchi sono divisi in:
				\begin{itemize}
					\item \textbf{data region} dove si contengono i dati utente (\textit{gran parte della memoria}).
					\item \textbf{inodes table} dove si contengono gli inodes (\textit{il numero di inodes rappresenta il numero massimo di files che possono essere mantenuti da VSFS}).
					\item \textbf{allocation structure} per tenere traccia di quando gli inodes o i data-blocks sono liberi. Vengono divise in
					\begin{itemize}
						\item \textbf{data bitmap}
						\item \textbf{inodes bitmap}
					\end{itemize}
					\item \textbf{superblock} è un singolo blocco che contiene informazioni relative al file system stesso (numero inodes, data blocks, \dots). Questo blocco include anche un magic number per identificare il tipo di file system.
				\end{itemize}				
				La bitmap è una struttura piuttosto semplice, ogni bit viene usato per indicare quando il corrispondete oggetto/blocco è libero (0) o occupato (1).
				
			\subsubsection{Organizzazione dei file, l'inode}
				Gli \textbf{inodes} (\textit{index node}) sono strutture adoperate da tutti file system. Ogni inode viene implicitamente riferito da un (\textbf{inumber}) di un file. In VSFS, dato un inumber \texttt{i}, per calcolare dove l'inode corrispondente è collocato su disco si fa: $$\texttt{i * (dimensione inode) + (indirizzo inizio inode region)}$$
				I dischi non sono indirizzabili attraverso i byte. Consistono in un vasto numero di settori indirizzabili, generalmente da \texttt{512B}. 
				$$\texttt{blk = inumber * sizeof(inode\_t) / blockSize}$$
				$$\texttt{sector = ((blk * blockSize) + inodeStartAddr) / sectorSize}$$
				All'interno di un inode troviamo informazioni quali: 
				\begin{itemize}
					\item \textbf{Tipo} di file (\textit{directory, normale o link}).
					\item \textbf{Dimensione}
					\item \textbf{Numero di data-blocks}
					\item \textbf{Informazioni relative alla protezione}.
					\item \textbf{Informazioni relative al tempo} (\textit{quando è stato creato, modificato, ultimo accesso, \dots}).
				\end{itemize}
				Gli inodes possono riferirsi ai data blocks in due modi:
				\begin{itemize}
					\item \textbf{direct pointers} (disk addresses) all'interno dell'inode; ogni puntatore fa riferimento a un blocco su disco appartenente al file. Questo approccio è limitato se si vuole creare un file più grande della dimensione di un blocco moltiplicato per il numero di direct pointers.
					\item \textbf{indirect pointers} Invece di puntare a un blocco che contiene dati utente, punta a un blocco che contiene più puntatori, i quali puntano ai dati utente. In questo modo, un inode potrebbe avere un numero fissato di direct pointers (\textit{esempio 12}) e un singolo indirect pointer. Se il file cresce abbastanza, viene allocato un indirect block (dalla data-block region del disco) e lo slot dell'inode relativo all'indirect pointer è impostato in modo tale che punti a esso. Questa organizzazione dei puntatori è un sistema di indicizzazione che prende il nome di \textbf{multi-level index}. 
				\end{itemize}
				Per supportare file di dimensione ancora maggiore, basta aggiungere un altro puntatore all'inode: \textbf{double indirect pointer}. Questo puntatore si riferisce a un blocco che contiene puntatori a blocchi indiretti, ognuno dei quali contiene puntatori a data blocks. È possibile usare anche un \textbf{triple indirect pointer} se si vuole. Attraverso questo sistema di indicizzazione, siamo in grado di supportare file di dimensioni maggiori. I tipi di multi-level index introdotto sono:
				\begin{itemize}
					\item \textbf{Direct indexing.} Il file può essere grande massimo 12 blocchi, ciascuno di dimensione \texttt{4K}.
					\item \textbf{Single indirect indexing.} Il puntatore punta a un blocco che contiene altri puntatori. Ciascuno di questi punta a un blocco che contiene dati utente. Avendo puntatori da \texttt{4B} si ha che un blocco di puntatori suddiviso in fette da \texttt{4B} (1024 indirizzi da \texttt{4B} ciascuno, un file può essere grande \texttt{(12 + 1024) * 4KB = 4144KB}).
					\item \textbf{Double indirect indexing} il puntatore punta a un blocco di puntatori, ciascuno dei quali punta a un altro blocco di puntatori, ciascuno dei quali, infine, punta a un blocco che contiene i dati utente. Il file può essere grande \texttt{4GB}.
					\item \textbf{Triple indirect indexing} in questo caso riesco a indicizzare un numero enorme di blocchi, riuscendo a supportare file di dimensioni notevoli. Nello specifico si ha \texttt{(12 + 1024 + $1024^2$ + $1024^3$ )*4 KB = 4 TB.}
				\end{itemize}
				Il motivo per cui si utilizza un albero così sbilanciato è perchè la maggior parte delle volte i file sono di dimensione piccola. Viene ottimizzato il caso più frequente.
			
			\subsubsection{Organizzazione delle directories}
				In VSFS il contenuto di una directory è una coppia \textbf{entry name, inode number}. Le directories vengono spesso trattate dai file system come tipo di files speciali. In questo modo, una directory avrà un inode da qualche parte nell inode table (con il campo \texttt{type} dell'inode marcato come directory.
				
			\subsubsection{Free space management}
				La gestione dello spazio libero è molto importante, in VSFS ci sono due bitmap che fanno ciò.
				Se creiamo un file, dobbiamo allocare un inode per esso. Il file system cercherà nella bitmap un inode libero, lo setterà a 1 e lo allocherà al file. Un procedimento simile viene adoperato per quanto riguarda i blocchi di dati.
				
			\subsubsection{Reading to disk}
				A seguito della syscall \texttt{open()} il file system dovrà trovare l'inode del file avendo a disposizione solamente il pathname completo, per ottenere informazioni base riguardo a esso (\textit{dimensione, permessi, \dots}). Tutti gli attraversamenti partono dalla directory \textbf{root}. 
				
			\subsubsection{Writing to disk}
				Scrivere su un file è un processo simile a quello descritto in precedenza. Per prima cosa è necessario aprire il file invocando la syscall \texttt{open()}. Quindi, l’applicazione può invocare la \texttt{write()} per aggiornarne il contenuto. Infine, il file viene chiuso tramite la syscall \texttt{close()}. 
				
			\subsubsection{Caching e buffering}
				Per evitare che le prestazioni calino eccessivamente, i file systems usano spesso la memoria centrale (DRAM) in modo aggressivo, per "nascondere/far sostare" blocchi "importanti". I primi file systems introdussero una fixed-size cache con lo scopo di mantenere accessibili in modo rapido blocchi "popolari" (di uso frequente). Algoritmi quali LRU, decidevano quale blocco mantenere nel buffer. La file-cache veniva generalmente allocata a boot-time per essere approssimativamente il 10\% della memoria totale. Nei moderni sistemi operativi viene utilizzato un metodo differente, noto come partizionamento dinamico. Vengono integrate pagine di memoria virtuale e pagine di file system in una unified-page-cache. Con questo approccio la memoria può essere allocata in modo più flessibile attraverso la memoria virtuale e il file system, in base alle necessità in un certo istante di tempo.
				
		
		\subsection{Crash consistency}
			Se durante l'aggiornamento del file system si verifica una perdita di energia o un crash del sistema, questo è noto come \textbf{crash-consistency problem}. Supponiamo di scrivere in un file. Per farlo, dobbiamo eseguire tre write: aggiornare l'inode, aggiungere un nuovo bloco nella regione dati e aggiornare la data bitmap. Se una di quete write non viene completata per via di un crash, il file system viene lasciato in uno stato "inconsistente". 
			
			\subsubsection{Crash scenarios}
				\begin{itemize}
					\item \textbf{Solamente il data-block è stato scritto su disco} Non rappresenta un problema per la crash consistency perchè è come se il blocco non fosse mai esistito. L'utente potrebbe perdere però dei dati importanti.
					\item \textbf{Solamente l'inode viene scritto su disco} e punterà a uno o più data blocks su disco dentro ai quali non è stata inserita alcuna informazione; essi conterranno \textbf{garbage data} del disco. C'è \textbf{file system inconsistency} in quanto l'inode punta al blocco dati mentre la bitmap lo ha segnato come libero.
					\item \textbf{Solamente la bitmap viene scritta su disco} La bitmap indica che il blocco è occupato ma non si ha nessun inode che punta ad esso (\textbf{inconsistency}). Ciò porta a dello \textbf{space leak}.
					\item \textbf{Inode e bitmap sono scritti su disco} Il blocco a cui punta l'inode contiene ancora una volta \textbf{garbage data}. Ciò non scaturisce file system incosistency.
					\item \textbf{Inode e blocco vengono scritti su disco} Il blocco non potrà essere acceduto e rischia di essere sovrascritto perchè marcato come libero nella data bitmap.
					\item \textbf{Bitmap e blocco vengono scritti su disco} Il blocco non sarà accessibile poichè nessun inode punterà ad esso. Non sappiamo quale sia il file associato a tale blocco.
				\end{itemize}
				
			\subsubsection{FSCK - File System Checker}
				FSCK serve per trovare inconsistenze del file system e ripararle. Questo tool viene eseguito prima che il file system venga montato e reso disponibile. Una volta finito il lavoro, questo programma dovrebbe rendere il file system consistente e accessibile all'utente. FSCK esegue sequenzialmente le seguenti fasi:
				\begin{itemize}
					\item \textbf{Sanity check} Controlla che il superblock sia corretto controllando ad esempio che la grandezza del file system sia maggiore del numero di blocchi allocati e così via. L'obiettivo è trovare un superblock corrotto e sostituirlo con una copia funzionante.
					\item \textbf{Free blocks} Scansiona gli inodes, indirect blocks, double indirect blocks e così via. In questo modo produce una versione corretta della data bitmap (fidandosi delle informazioni contenute negli inodes). La stessa scansione viene fatta per gli inodes e la loro relativa bitmap.
					\item \textbf{Inode state} Ogni inode viene controllato per verificare se corrotto o se ha altri problemi. Se l'inode è ritenuto sospetto e ha problemi che non possono essere risolti facilmente, viene pulito e, successivamente, viene aggiornata la bitmap.
					\item \textbf{Inode links} Controlla il link count di ogni inode allocato e costruisce il proprio link count per ogni directory presente. Se il link count di FSCK è differente da quello del file system per una data directory si procede ad aggiornare quest'ultimo col valore corretto.
					\item \textbf{Duplicates} Controlla che non ci siano puntatori duplicati. Se un inode è corrotto, potrebbe venir pulito. Alternativamente, viene copiato il blocco, in questo modo ogni inode ha la propria copia.
					\item \textbf{Bad blocks} Viene fatto un controllo anche per i puntatori a blocchi possibilmente corrotti. Un puntatore è considerato corrotto se punta al di fuori di un range valido, e venogno rimossi i puntatori dall'inode o dagli indirect blocks.
					\item \textbf{Directory checks} Viene eseguito un controllo di integrità sulle directories. FSCK controlla contenuto di ogni directory, assicurandosi che "." e ".." siano le prime entries e che ogni inode riferito a una entry di una directory sia allocato correttamente. Si assicura che non ci siano cicli.
				\end{itemize}
				Il problema di questo approccio è che è \textbf{estremamente lento}.
				
			\subsubsection{Journaling}
				L'idea base è la seguente: prima di aggiornare le strutture dati su disco, viene generata una "nota" (da qualche parte nel disco in una struttura conosciuta come log) che descrive l'operazione che il file system è in procinto di svolgere (\textbf{write-ahead}). In questo modo, se si dovesse verificare un crash durante l'aggiornamento delle strutture dati, è sempre possibile andare a consultare la nota e sapere esattamente cosa sistemare (e come). Dopo un crash siamo in grado di andare direttamente alla fonte del problema.
				
				Nel file system \texttt{linux ext3} la nuova struttura chiave è il blocco \textbf{journal}, il quale occupa un piccolo spazio all'interno della partizione. Supponiamo di voler eseguire l'aggiornamento: \texttt{inode (I[v2]), bitmap (B[v2]), data block (Db)}. Prima di scrivere su disco dobbiamo scrivere nel log (journal) le informazioni necessarie. Oltre ai tre soliti blocchi, si scrivono i blocchi \texttt{TxB} (\textit{transaction begin}) all'inizio e \texttt{TxE} (\textit{transaction end}) alla fine. \texttt{TxB} contiene informazioni relative all'aggiornamento in attesa, tra cui, ad esempio, l'indirizzo finale dei blocchi oltre a un qualche tipo di \textbf{transaction identifier (TID)}. Il blocco finale, \texttt{TxE} è un marcatore della fine della transazione e conterrà anch'esso il TID. Una volta che il blocco journal risiede su disco, è possibile andare ad eseguire l'aggiornamento delle vecchie strutture dati. Questo processo è chiamato \textbf{checkpointing}. La sequenza delle operazioni svolte è:
				\begin{itemize}
					\item \textbf{Journal write} vengono inserite le informazioni necessarie nel blocco journal.
					\item \textbf{Checkpoint} vengono apportate le opportune modifiche su disco, alle locazioni corrette. 
				\end{itemize}
				Se dovesse verificarsi un crash mentre stiamo scrivendo i dati nel blocco journal, per evitare di scrivere \texttt{garbage data}, il file system esegue la scritture in due steps. Per prima cosa scrive tutti i blocchi tranne \texttt{TxE} nel journal. Quando la scrittura di questi blocchi verrà portata a termine, il file system può procedere a scrivere anche \texttt{TxE}. Il punto chiave di questo approccio è l'\textbf{atomicità}. Il disco garantisce che le scritture da \texttt{512B} vengano eseguite atomicamente. Il blocco \texttt{TxE} dovrebbe quindi essere grande \texttt{512B} per fare in modo che funzioni correttamente. Il protoccolo di aggiornamento del file system sarà quindi:
				\begin{itemize}
					\item \textbf{Journal write} viene scritto il contenuto della transazione nel log (\textit{TxB,metadati e dati}).
					\item \textbf{Journal commit} Viene scritto il \textit{commit block }della transazione \texttt{TxE} nel log e si attende il completamento di questa write.
					\item \textbf{Checkpoint} Viene applicato l'aggiornamento delle opportune locazioni su disco.
				\end{itemize}		
			
			\subsubsection{Recovery}	
				Se il crash avviene prima che la scrittura della transazione sia completa, l'update in attesa viene semplicemente saltato. Se il crash avviene dopo che la transazione ha scritto nel \textit{commit block} ma prima che il checkpoint sia completato, il file system può riprendere l'aggiornamento: quando il sistema viene avviato, il recovery process del file system scansiona il log e cerca le transazioni che hanno provato ad accedere a disco (committed). Queste transazioni vengono eseguite nuovamente in ordine, nel tentativo di aggiornare le opportune strutture dati. Questo processo di recovery viene chiamato \textbf{redo-logging}. Se il crash avviene durante il checkpoint possiamo sempre leggere il contenuto del journal durante il recovery ed eseguire nuovamente l'aggiornamento (le writes vengono rieseguite tutte).\\\\
				Il protocollo introdotto potrebbe generare traffico I/O addizionale. 
				
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
	\section{JOS}


\end{document}
